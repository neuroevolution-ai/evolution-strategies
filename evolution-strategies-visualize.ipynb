{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import roboschool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gym import wrappers\n",
    "from ipywidgets import Video\n",
    "from multiprocessing import Pool, Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model\n",
    "\n",
    "Since we use a custom initializer and this gets serialized during the saving process of the model we need to pass it on when we load it again. Unfortunately with the issue of the background TensorFlow session when importing TensorFlow and multiprocessing we cannot define the initializer one time and use it here again. So we define it twice, one inside the create_model() function and here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):   \n",
    "    import tensorflow as tf\n",
    "\n",
    "    class Normc_initializer(tf.keras.initializers.Initializer):\n",
    "        \"\"\"\n",
    "        Create a TensorFlow constant with random numbers normed in the given shape.\n",
    "        :param std:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def __init__(self, std=1.0):\n",
    "            self.std = std\n",
    "\n",
    "        def __call__(self, shape, dtype=None, partition_info=None):\n",
    "            out = np.random.randn(*shape).astype(np.float32)\n",
    "            out *= self.std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "            return tf.constant(out)\n",
    "        \n",
    "    class ObservationNormalizationLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, ob_mean, ob_std, **kwargs):\n",
    "            self.ob_mean = ob_mean\n",
    "            self.ob_std = ob_std\n",
    "            super(ObservationNormalizationLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def call(self, input):\n",
    "            return tf.clip_by_value((input - self.ob_mean) / self.ob_std, -5.0, 5.0)\n",
    "          \n",
    "        def get_config(self):\n",
    "            base_config = super(ObservationNormalizationLayer, self).get_config()\n",
    "            base_config['ob_mean'] = self.ob_mean\n",
    "            base_config['ob_std'] = self.ob_std\n",
    "            return base_config\n",
    "        \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "        \n",
    "    class DiscretizeActionsUniformLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_ac_bins, adim, ahigh, alow, **kwargs):\n",
    "            self.num_ac_bins = num_ac_bins\n",
    "            self.adim = adim\n",
    "            self.ahigh = ahigh\n",
    "            self.alow = alow\n",
    "            super(DiscretizeActionsUniformLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def call(self, x):\n",
    "            scores = tf.keras.layers.Dense(\n",
    "                            self.adim * self.num_ac_bins,\n",
    "                            kernel_initializer=Normc_initializer(std=0.01),\n",
    "                            bias_initializer=tf.initializers.zeros())(x)\n",
    "            \n",
    "            # Reshape to [n x i x j] where n is dynamically chosen, i equals action dimension and j equals the number\n",
    "            # of bins\n",
    "            scores_nab = tf.reshape(scores, [-1, self.adim, self.num_ac_bins])\n",
    "            # This picks the bin with the greatest value\n",
    "            a = tf.argmax(scores_nab, 2)\n",
    "            \n",
    "            # Then transform the interval from [0, num_ac_bins - 1] to [-1, 1] which equals alow and ahigh\n",
    "            ac_range_1a = (self.ahigh - self.alow)[None, :]\n",
    "            return 1. / (self.num_ac_bins - 1.) * tf.keras.backend.cast(a, 'float32') * ac_range_1a + self.alow[None, :]        \n",
    "        \n",
    "        # get_config and from_config need to implemented to be able to serialize the model\n",
    "        def get_config(self):\n",
    "            base_config = super(DiscretizeActionsUniformLayer, self).get_config()\n",
    "            base_config['num_ac_bins'] = self.num_ac_bins\n",
    "            base_config['adim'] = self.adim\n",
    "            base_config['ahigh'] = self.ahigh\n",
    "            base_config['alow'] = self.alow\n",
    "            return base_config\n",
    "        \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "    \n",
    "    custom_objects = {'Normc_initializer' : Normc_initializer, \n",
    "                      'ObservationNormalizationLayer' : ObservationNormalizationLayer,\n",
    "                      'DiscretizeActionsUniformLayer' : DiscretizeActionsUniformLayer}\n",
    "    \n",
    "    return tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "def rollout(env, model, render=False, timestep_limit=None, random_stream=None):\n",
    "    \"\"\"\n",
    "    If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "    Otherwise, no action noise will be added.\n",
    "    \"\"\"\n",
    "\n",
    "    env_timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "    rews = []\n",
    "    t = 0\n",
    "\n",
    "    ob = env.reset()\n",
    "    for _ in range(timestep_limit):\n",
    "        if render:\n",
    "            env.render()\n",
    "        ac = act(ob[None], model, random_stream=random_stream)[0]\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "        rews.append(rew)\n",
    "        t += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return np.array(rews, dtype=np.float32), t\n",
    "\n",
    "def act(ob, model, random_stream=None):   \n",
    "    action = model.predict(ob)\n",
    "    \n",
    "    #if random_stream is not None and model_structure.ac_noise_std != 0:\n",
    "    #    action += random_stream.randn(*action.shape) * model_structure.ac_noise_std\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generation_number(model_file_path):\n",
    "    try: \n",
    "        number = int(model_file_path.split('snapshot_')[-1].split('.h5')[0])\n",
    "        return number\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "def index_save_directory(save_directory):\n",
    "    if not os.path.isdir(save_directory):\n",
    "        return None, None, None\n",
    "        \n",
    "    model_file_paths, log_file_path, config_file = [], None, None\n",
    "    \n",
    "    for file in os.listdir(save_directory):\n",
    "        if file.endswith('.h5'):\n",
    "            model_file_paths.append(file)\n",
    "        elif file.endswith('config.json'):\n",
    "            with open(save_directory + file, 'r') as f:\n",
    "                config_file = json.load(f)\n",
    "        elif file.endswith('log.txt'):\n",
    "            log_file_path = save_directory + file\n",
    "    \n",
    "    model_file_paths.sort()\n",
    "    \n",
    "    return model_file_paths, log_file_path, config_file\n",
    "\n",
    "def run_model(env_id, save_directory, model_file_path, record=False):\n",
    "    env = gym.make(env_id)\n",
    "    env.reset()\n",
    "    \n",
    "    if record:\n",
    "        video_directory = save_directory + 'videos/' + model_file_path + '/'\n",
    "        env = wrappers.Monitor(env, video_directory, force=True)\n",
    "        \n",
    "    model = load_model(save_directory + model_file_path)\n",
    "    \n",
    "    rewards, length = rollout(env, model)\n",
    "    \n",
    "    if record:\n",
    "        return video_directory\n",
    "    \n",
    "    return [rewards.sum(), length]\n",
    "\n",
    "def visualize_model(save_directory, model_file_path, config_file):\n",
    "                                       \n",
    "    with Pool(os.cpu_count()) as pool:\n",
    "        video_directory = pool.apply(func=run_model, args=(config_file['config']['env_id'],\n",
    "                                                           save_directory,\n",
    "                                                           model_file_path,\n",
    "                                                           True))\n",
    "    \n",
    "    for file in os.listdir(video_directory):\n",
    "        if file.endswith('.mp4'):\n",
    "            return video_directory + file\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_to_csv(log_file, csv_file):\n",
    "    with open(log_file) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    groups = temp =  []\n",
    "    for line in content:\n",
    "        line = line.split()\n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if \"Generation\" in line:\n",
    "            temp = [line[-1]]\n",
    "            groups.append(temp)\n",
    "        else:\n",
    "            temp.append(line[-1])\n",
    "\n",
    "    writer = csv.writer(open(csv_file, 'w'))\n",
    "\n",
    "    writer.writerow(['Generation',\n",
    "                     'Rew_Mean',\n",
    "                     'Rew_Std',\n",
    "                     'Len_Mean',\n",
    "                     'Eval_Rew_Mean',\n",
    "                     'Eval_Rew_Std',\n",
    "                     'Eval_Len_Mean',\n",
    "                     'Eval_Count',\n",
    "                     'Episodes_this_Gen',\n",
    "                     'Episodes_overall',\n",
    "                     'Timesteps_this_gen',\n",
    "                     'Timesteps_overall',\n",
    "                     'Unique_Workers',\n",
    "                     'ResultsSkippedFrac',\n",
    "                     'Observation_count',\n",
    "                     'Time_elapsed_this_Gen',\n",
    "                     'Time_elapsed_overall',\n",
    "                     'TimePerMutationMin',\n",
    "                     'TimePerMutationMax',\n",
    "                     'TimePerMutationMean',\n",
    "                     'TimePerMutationCount',\n",
    "                     'TimeCreateModelMin',\n",
    "                     'TimeCreateModelMax',\n",
    "                     'TimeCreateModelMean',\n",
    "                     'TimeCreateModelCount',                     \n",
    "                     'TimeSetFlatMin',\n",
    "                     'TimeSetFlatMax',\n",
    "                     'TimeSetFlatMean',\n",
    "                     'TimeSetFlatCount',\n",
    "                     'TimeSampleMin',\n",
    "                     'TimeSampleMax',\n",
    "                     'TimeSampleMean',\n",
    "                     'TimeSampleCount',\n",
    "                     'TimeGetNoiseMin',\n",
    "                     'TimeGetNoiseMax',\n",
    "                     'TimeGetNoiseMean',\n",
    "                     'TimeGetNoiseCount',\n",
    "                     'TimePredictMin',\n",
    "                     'TimePredictMax',\n",
    "                     'TimePredictMean',\n",
    "                     'TimePredictCount'])\n",
    "\n",
    "    for generation in groups:\n",
    "        if len(generation) != 43: continue\n",
    "\n",
    "        # Throw out save_directory and distinction line\n",
    "        generation = generation[:41]\n",
    "        row = []\n",
    "        \n",
    "        for column in generation:\n",
    "            row.append(column)\n",
    "\n",
    "        writer.writerow(row)\n",
    "\n",
    "def evaluate_to_csv(save_directory, model_file_paths, config_file, csv_eval_file_path, eval_count=5):\n",
    "    writer = csv.writer(open(csv_eval_file_path, 'w'))\n",
    "    \n",
    "    head_row = ['Generation',\n",
    "                'Eval_per_Gen',\n",
    "                'Eval_Rew_Mean', \n",
    "                'Eval_Rew_Std', \n",
    "                'Eval_Len_Mean']\n",
    "    \n",
    "    for i in range(eval_count):\n",
    "        head_row.append('Rew_' + str(i))\n",
    "        head_row.append('Len_' + str(i))\n",
    "        \n",
    "    writer.writerow(head_row)\n",
    "        \n",
    "    for model_file_path in model_file_paths:\n",
    "        results = []\n",
    "        with Pool(os.cpu_count()) as pool:\n",
    "            for _ in range(eval_count):\n",
    "                results.append(pool.apply_async(func=run_model, args=(config_file['config']['env_id'], \n",
    "                                                                      save_directory, \n",
    "                                                                      model_file_path)))\n",
    "                \n",
    "            for i in range(len(results)):\n",
    "                results[i] = results[i].get()\n",
    "        \n",
    "        rewards = np.array(results)[:, 0]\n",
    "        lengths = np.array(results)[:, 1]\n",
    "        \n",
    "        row = [parse_generation_number(model_file_path),\n",
    "               eval_count,\n",
    "               np.mean(rewards),\n",
    "               np.std(rewards),\n",
    "               np.mean(lengths)]\n",
    "        \n",
    "        assert len(rewards) == len(lengths)\n",
    "        for i in range(len(rewards)):\n",
    "            row.append(rewards[i])\n",
    "            row.append(lengths[i])\n",
    "        \n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_count = 5\n",
    "\n",
    "save_directory = '/tmp/es_25675/'\n",
    "\n",
    "if not save_directory.endswith('/'):\n",
    "    save_directory += '/'\n",
    "    \n",
    "model_file_paths, log_file_path, config_file = index_save_directory(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_file_paths is None or config_file is None:\n",
    "    print(\"Not enough data to evaluate the training. Please provide a directory with enough data.\")\n",
    "else:\n",
    "    if log_file_path is not None:    \n",
    "        # Save parsed log to a csv spreadsheet\n",
    "        csv_log_file_path = save_directory + 'log.csv'\n",
    "        parse_log_to_csv(log_file_path, csv_log_file_path)\n",
    "    \n",
    "    csv_eval_file_path = save_directory + 'evaluation.csv'\n",
    "    evaluate_to_csv(save_directory, model_file_paths, config_file, csv_eval_file_path, eval_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = model_file_paths[-1]\n",
    "video_file = visualize_model(save_directory, model_file_path, config_file)\n",
    "\n",
    "if video_file is not None:\n",
    "    video = Video.from_file(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = pd.read_csv(save_directory + 'evaluation.csv')\n",
    "log_data = pd.read_csv(save_directory + 'log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(eval_data.Generation, eval_data.Eval_Rew_Mean)\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Cummulative reward')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import roboschool\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gym import wrappers\n",
    "from ipywidgets import Video\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model\n",
    "\n",
    "Since we use a custom initializer and this gets serialized during the saving process of the model we need to pass it on when we load it again. Unfortunately with the issue of the background TensorFlow session when importing TensorFlow and multiprocessing we cannot define the initializer one time and use it here again. So we define it twice, one inside the create_model() function and here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):   \n",
    "    import tensorflow as tf\n",
    "\n",
    "    class Normc_initializer(tf.keras.initializers.Initializer):\n",
    "        \"\"\"\n",
    "        Create a TensorFlow constant with random numbers normed in the given shape.\n",
    "        :param std:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def __init__(self, std=1.0):\n",
    "            self.std = std\n",
    "\n",
    "        def __call__(self, shape, dtype=None, partition_info=None):\n",
    "            out = np.random.randn(*shape).astype(np.float32)\n",
    "            out *= self.std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "            return tf.constant(out)\n",
    "        \n",
    "    class ObservationNormalizationLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, ob_mean, ob_std, **kwargs):\n",
    "            self.ob_mean = ob_mean\n",
    "            self.ob_std = ob_std\n",
    "            super(ObservationNormalizationLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def call(self, input):\n",
    "            return tf.clip_by_value((input - self.ob_mean) / self.ob_std, -5.0, 5.0)\n",
    "          \n",
    "        def get_config(self):\n",
    "            base_config = super(ObservationNormalizationLayer, self).get_config()\n",
    "            base_config['ob_mean'] = self.ob_mean\n",
    "            base_config['ob_std'] = self.ob_std\n",
    "            return base_config\n",
    "        \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "    \n",
    "    custom_objects = {'Normc_initializer' : Normc_initializer(std=1.0), \n",
    "                      'ObservationNormalizationLayer' : ObservationNormalizationLayer}\n",
    "    \n",
    "    return tf.keras.models.load_model(model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, model, render=False, timestep_limit=None, random_stream=None):\n",
    "    \"\"\"\n",
    "    If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "    Otherwise, no action noise will be added.\n",
    "    \"\"\"\n",
    "\n",
    "    env_timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "    rews = []\n",
    "    t = 0\n",
    "\n",
    "    ob = env.reset()\n",
    "    for _ in range(timestep_limit):\n",
    "        if render:\n",
    "            env.render()\n",
    "        ac = act(ob[None], model, random_stream=random_stream)[0]\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "        rews.append(rew)\n",
    "        t += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return np.array(rews, dtype=np.float32), t\n",
    "\n",
    "def act(ob, model, random_stream=None):   \n",
    "    action = model.predict(ob)\n",
    "    \n",
    "    #if random_stream is not None and model_structure.ac_noise_std != 0:\n",
    "    #    action += random_stream.randn(*action.shape) * model_structure.ac_noise_std\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _visualize_in_process(env_id, model_file, save_directory, record=False):\n",
    "    env = gym.make(env_id)\n",
    "    env.reset()\n",
    "    \n",
    "    video_directory = save_directory + '/videos/' + model_file + '/'\n",
    "    \n",
    "    if record:\n",
    "        env = wrappers.Monitor(env, video_directory, force=True)\n",
    "        \n",
    "    model = load_model(save_directory + model_file)\n",
    "    \n",
    "    rollout(env, model)\n",
    "    \n",
    "    return video_directory\n",
    "\n",
    "def visualize(save_directory):\n",
    "    files = [save_directory + file for file in os.listdir(save_directory)]\n",
    "    \n",
    "    model_files, config_files = [], {}\n",
    "    \n",
    "    for file in os.listdir(save_directory):\n",
    "        if file.endswith('.h5'):\n",
    "            model_files.append(file)\n",
    "        elif file.endswith('.json'):\n",
    "            with open(save_directory + file, 'r') as f:\n",
    "                config_files = json.load(f)\n",
    "    \n",
    "    model_files.sort()\n",
    "\n",
    "    video_directories = []\n",
    "\n",
    "    with Pool(os.cpu_count()) as pool:\n",
    "        for m in model_files:\n",
    "            video_directories.append(pool.apply_async(func=_visualize_in_process, args=(config_files['config']['env_id'], \n",
    "                                                        m,\n",
    "                                                        save_directory,\n",
    "                                                        True)))\n",
    "        \n",
    "        for i in range(len(video_directories)):\n",
    "            video_directories[i] = video_directories[i].get()\n",
    "    \n",
    "    return video_directories        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = '/tmp/es_2702/'\n",
    "\n",
    "video_directories = visualize(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = []\n",
    "for directory in video_directories:\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.mp4'):\n",
    "            videos.append(Video.from_file(directory + file))\n",
    "            \n",
    "for video in videos:\n",
    "    video\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_to_csv(log_file, csv_file):\n",
    "    with open(log_file) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    groups = temp =  []\n",
    "    for line in content:\n",
    "        line = line.split()\n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if \"Generation\" in line:\n",
    "            temp = [line[-1]]\n",
    "            groups.append(temp)\n",
    "        else:\n",
    "            temp.append(line[-1])\n",
    "\n",
    "    writer = csv.writer(open(csv_file, 'w'))\n",
    "\n",
    "    writer.writerow(['Generation',\n",
    "                     'Reward Mean',\n",
    "                     'Reward Standard Deviation',\n",
    "                     'Length Mean',\n",
    "                     'Evaluation Reward Mean',\n",
    "                     'Evaluation Reward Standard Deviation',\n",
    "                     'Evaluation Length Mean',\n",
    "                     'Evaluation Count',\n",
    "                     'Episodes this generation',\n",
    "                     'Episodes overall',\n",
    "                     'Timesteps this generation',\n",
    "                     'Timesteps overall',\n",
    "                     'Unique Workers',\n",
    "                     'Observation count',\n",
    "                     'Time elapsed this generation (s)',\n",
    "                     'Time elapsed overall (s)'])\n",
    "\n",
    "    for generation in groups:\n",
    "        if len(generation) != 18: continue\n",
    "\n",
    "        row = []\n",
    "\n",
    "        for column in generation:\n",
    "            row.append(column)\n",
    "\n",
    "        writer.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import roboschool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gym import wrappers\n",
    "from ipywidgets import Video\n",
    "import ipywidgets as widgets\n",
    "from multiprocessing import Pool, Process\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%javascript\n",
    "#IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model\n",
    "\n",
    "Since we use a custom initializer and this gets serialized during the saving process of the model we need to pass it on when we load it again. Unfortunately with the issue of the background TensorFlow session when importing TensorFlow and multiprocessing we cannot define the initializer one time and use it here again. So we define it twice, one inside the create_model() function and here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):   \n",
    "    import tensorflow as tf\n",
    "\n",
    "    class Normc_initializer(tf.keras.initializers.Initializer):\n",
    "        \"\"\"\n",
    "        Create a TensorFlow constant with random numbers normed in the given shape.\n",
    "        :param std:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def __init__(self, std=1.0):\n",
    "            self.std = std\n",
    "\n",
    "        def __call__(self, shape, dtype=None, partition_info=None):\n",
    "            out = np.random.randn(*shape).astype(np.float32)\n",
    "            out *= self.std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "            return tf.constant(out)\n",
    "        \n",
    "    class ObservationNormalizationLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, ob_mean, ob_std, **kwargs):\n",
    "            self.ob_mean = ob_mean\n",
    "            self.ob_std = ob_std\n",
    "            super(ObservationNormalizationLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def call(self, input):\n",
    "            return tf.clip_by_value((input - self.ob_mean) / self.ob_std, -5.0, 5.0)\n",
    "          \n",
    "        def get_config(self):\n",
    "            base_config = super(ObservationNormalizationLayer, self).get_config()\n",
    "            base_config['ob_mean'] = self.ob_mean\n",
    "            base_config['ob_std'] = self.ob_std\n",
    "            return base_config\n",
    "        \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "        \n",
    "    class DiscretizeActionsUniformLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_ac_bins, adim, ahigh, alow, **kwargs):\n",
    "            self.num_ac_bins = num_ac_bins\n",
    "            self.adim = adim\n",
    "            self.ahigh = ahigh\n",
    "            self.alow = alow\n",
    "            super(DiscretizeActionsUniformLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def call(self, x):            \n",
    "            # Reshape to [n x i x j] where n is dynamically chosen, i equals action dimension and j equals the number\n",
    "            # of bins\n",
    "            scores_nab = tf.reshape(x, [-1, self.adim, self.num_ac_bins])\n",
    "            # This picks the bin with the greatest value\n",
    "            a = tf.argmax(scores_nab, 2)\n",
    "            \n",
    "            # Then transform the interval from [0, num_ac_bins - 1] to [-1, 1] which equals alow and ahigh\n",
    "            ac_range_1a = (self.ahigh - self.alow)[None, :]\n",
    "            return 1. / (self.num_ac_bins - 1.) * tf.keras.backend.cast(a, 'float32') * ac_range_1a + self.alow[None, :]        \n",
    "        \n",
    "        # get_config and from_config need to implemented to be able to serialize the model\n",
    "        def get_config(self):\n",
    "            base_config = super(DiscretizeActionsUniformLayer, self).get_config()\n",
    "            base_config['num_ac_bins'] = self.num_ac_bins\n",
    "            base_config['adim'] = self.adim\n",
    "            base_config['ahigh'] = self.ahigh\n",
    "            base_config['alow'] = self.alow\n",
    "            return base_config\n",
    "        \n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "    \n",
    "    custom_objects = {'Normc_initializer' : Normc_initializer, \n",
    "                      'ObservationNormalizationLayer' : ObservationNormalizationLayer,\n",
    "                      'DiscretizeActionsUniformLayer' : DiscretizeActionsUniformLayer}\n",
    "    \n",
    "    return tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "def rollout_evaluation(env, model, render=False, timestep_limit=None, random_stream=None):\n",
    "    \"\"\"\n",
    "    If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "    Otherwise, no action noise will be added.\n",
    "    \"\"\"\n",
    "\n",
    "    env_timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "    rews = []\n",
    "    t = 0\n",
    "\n",
    "    ob = env.reset()\n",
    "    for _ in range(timestep_limit):\n",
    "        if render:\n",
    "            env.render()\n",
    "        ac = model.predict_on_batch(ob[None])[0]\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "        rews.append(rew)\n",
    "        t += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return np.array(rews, dtype=np.float32), t\n",
    "\n",
    "\n",
    "def run_model(model_file_path, model_file, save_directory, record=False):   \n",
    "    \n",
    "        with open(os.path.join(model_file_path, \"config.json\"), encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "    \n",
    "        env = gym.make(config['config']['env_id'])\n",
    "        env.reset()\n",
    "\n",
    "        if record:\n",
    "            env = wrappers.Monitor(env, save_directory, force=True)\n",
    "\n",
    "        model = load_model(os.path.join(model_file_path, model_file))\n",
    "\n",
    "        rewards, length = rollout_evaluation(env, model)\n",
    "        \n",
    "        print(rewards)\n",
    "        print([rewards.sum(), length])\n",
    "\n",
    "        return [rewards.sum(), length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "[-2.0050037  -1.9014888  -2.7920554  -3.4675457  -2.3709848  -1.8773048\n",
      " -1.772735   -1.2176484  -1.5918169  -1.4683728  -1.0826861  -2.4670074\n",
      " -2.8612452  -2.5890965  -1.6582769  -0.9889073  -1.5963849  -4.9361153\n",
      " -2.3565295  -2.8825014  -0.91857725 -1.988986   -0.9505365  -2.462678\n",
      " -1.0129569  -1.5374442  -2.0309856  -0.98935527 -0.9287383  -1.1476717\n",
      " -0.9792823  -0.8368079  -1.7926075  -0.433907   -1.2073644  -0.7464631\n",
      " -0.9094345  -0.7549313  -1.6941236  -0.6013869  -1.6459267  -1.634552\n",
      " -0.39750677 -2.3452377  -0.91469246 -0.27223393 -0.26072693 -1.2069854\n",
      " -0.47188237 -0.07824723 -0.70381075 -1.767184   -1.1963866  -0.7214007\n",
      " -2.5544195  -2.4491284  -1.894066   -0.70447135 -0.82842094 -2.3206706\n",
      " -0.6508348  -2.0189588  -1.6468253  -0.16195937 -0.9955688  -1.4073511\n",
      " -0.14046337 -0.37489876 -0.26046574 -0.12678288 -0.22753029 -0.36541733\n",
      " -1.5296091  -0.12430766  0.41385975  0.18224312  0.61031395  0.70674145\n",
      "  0.73700327  0.77187914  0.7840508   0.783282    0.78758097  0.79298747\n",
      "  0.79818684  0.81492954  0.81292874  0.77857023  0.87343186  0.8342288\n",
      "  0.8826117   0.856772    0.8963976   0.9185859   0.88976663  0.93715554\n",
      "  0.9288761   0.93153846  0.93594533  0.9334343   0.9331397   0.93166465\n",
      "  0.9310042   0.93043834  0.93043137  0.93032986  0.93010765  0.92982465\n",
      "  0.9295631   0.9293485   0.9291931   0.9290757   0.9289733   0.92887735\n",
      "  0.9287848   0.9287018   0.92863053  0.92856807  0.92851293  0.92846406\n",
      "  0.9284199   0.92837965  0.9283433   0.92830986  0.9282797   0.9282522\n",
      "  0.928227    0.9282034   0.9281817   0.928161    0.9281418   0.92812335\n",
      "  0.92810696  0.92809045  0.92807496  0.9280599   0.9280453   0.92803127\n",
      "  0.9280186   0.9280058   0.9279934   0.9279816   0.92797065  0.92795914\n",
      "  0.927949    0.9279386   0.9279289   0.9279193   0.9279101   0.9279016\n",
      "  0.92789257  0.9278845   0.9278761   0.9278687   0.92786115  0.92785376\n",
      "  0.92784667  0.92784023  0.9278338   0.9278268   0.9278206   0.92781466\n",
      "  0.92780924  0.9278036   0.9277981   0.927793    0.9277879   0.9277836\n",
      "  0.9277786   0.9277741   0.92776954  0.9277654   0.92776126  0.92775714\n",
      "  0.9277528   0.92774916  0.9277462   0.9277422   0.9277391   0.92773587\n",
      "  0.9277323   0.9277302   0.9277273   0.9277242   0.9277219   0.92771924\n",
      "  0.9277174   0.9277148   0.92771286  0.927711    0.9277089   0.9277069\n",
      "  0.927705    0.92770344  0.92770207  0.9277007   0.9276993   0.9276977\n",
      "  0.92769635  0.92769545  0.92769414  0.92769307  0.92769164  0.927691\n",
      "  0.9276902   0.9276891   0.9276883   0.9276877   0.9276868   0.92768604\n",
      "  0.92768556  0.9276854   0.9276846   0.92768407  0.92768365  0.9276835\n",
      "  0.92768323  0.9276831   0.92768234  0.92768264  0.92768246  0.92768234\n",
      "  0.9276822   0.9276822   0.9276826   0.9276825   0.9276827   0.927683\n",
      "  0.9276828   0.9276827   0.92768294  0.9276835   0.92768407  0.9276841\n",
      "  0.9276845   0.92768466  0.92768484  0.9276856   0.92768556  0.9276867\n",
      "  0.92768717  0.9276872   0.9276882   0.9276886   0.9276894   0.9276901\n",
      "  0.9276899   0.92769074  0.92769134  0.9276925   0.9276928   0.927693\n",
      "  0.927694    0.9276948   0.9276952   0.92769647  0.9276966   0.92769766\n",
      "  0.92769825  0.92769885  0.9277      0.9277008   0.9277012   0.92770207\n",
      "  0.9277029   0.9277037   0.9277044   0.92770517  0.92770636  0.92770684\n",
      "  0.9277077   0.927708    0.92770934  0.92771035  0.9277112   0.92771167\n",
      "  0.9277126   0.9277139   0.92771447  0.9277154   0.9277165   0.927717\n",
      "  0.92771786  0.9277184   0.92771983  0.92772055  0.92772156  0.9277224\n",
      "  0.9277233   0.92772406  0.92772484  0.92772585  0.92772686  0.92772746\n",
      "  0.92772824  0.92772925  0.9277303   0.92773116  0.92773175  0.9277332\n",
      "  0.92773354  0.92773443  0.92773527  0.92773646  0.9277378   0.92773783\n",
      "  0.92773885  0.92773986  0.9277405   0.92774177  0.9277428   0.9277433\n",
      "  0.9277439   0.9277448   0.9277456   0.92774683  0.9277478   0.92774844\n",
      "  0.9277492   0.92775035  0.9277509   0.9277519   0.92775244  0.9277533\n",
      "  0.9277543   0.927755    0.9277562   0.92775685  0.9277573   0.92775846\n",
      "  0.92775923  0.9277601   0.9277611   0.9277615   0.92776275  0.9277636\n",
      "  0.92776376  0.9277649   0.92776555  0.92776674  0.9277672   0.92776793\n",
      "  0.92776865  0.9277696   0.92777044  0.9277713   0.9277719   0.92777234\n",
      "  0.92777354  0.92777395  0.92777514  0.92777544  0.9277763   0.92777693\n",
      "  0.9277784   0.9277789   0.9277792   0.9277799   0.9277808   0.92778134\n",
      "  0.92778206  0.9277829   0.9277833   0.9277841   0.92778516  0.9277855\n",
      "  0.92778635  0.9277868   0.9277875   0.9277884   0.9277892   0.9277897\n",
      "  0.92779034  0.9277909   0.9277919   0.92779213  0.927793    0.9277932\n",
      "  0.92779386  0.927795    0.92779565  0.92779607  0.9277968   0.92779726\n",
      "  0.9277981   0.9277986   0.9277992   0.92780006  0.92780054  0.9278007\n",
      "  0.9278015   0.92780215  0.9278028   0.9278034   0.9278036   0.9278044\n",
      "  0.9278051   0.9278054   0.9278058   0.9278068   0.92780715  0.9278079\n",
      "  0.927808    0.9278085   0.92780924  0.9278098   0.92780995  0.92781097\n",
      "  0.9278115   0.92781216  0.92781264  0.92781276  0.9278133   0.9278137\n",
      "  0.92781407  0.92781466  0.9278149   0.92781574  0.92781615  0.9278171\n",
      "  0.92781705  0.9278179   0.927818    0.9278186   0.92781913  0.92781955\n",
      "  0.92781985  0.9278202   0.92782044  0.92782074  0.92782164  0.927822\n",
      "  0.92782223  0.92782295  0.92782307  0.9278235   0.9278238   0.92782426\n",
      "  0.9278246   0.9278249   0.9278256   0.92782587  0.9278263   0.9278265\n",
      "  0.9278268   0.92782706  0.92782766  0.92782784  0.9278283   0.9278288\n",
      "  0.92782897  0.92782944  0.9278303   0.9278301   0.92782986  0.92783064\n",
      "  0.9278308   0.92783123  0.92783165  0.92783165  0.9278322   0.9278322\n",
      "  0.9278328   0.927833    0.92783344  0.9278337   0.927834    0.9278342\n",
      "  0.92783475  0.92783463  0.927835    0.9278353   0.9278352   0.9278355\n",
      "  0.9278355   0.9278364   0.9278368   0.9278367   0.92783684  0.9278369\n",
      "  0.9278371   0.92783725  0.9278376   0.92783797  0.92783755  0.927838\n",
      "  0.92783827  0.9278387   0.9278388   0.9278389   0.9278394   0.92783934\n",
      "  0.9278397   0.9278397   0.9278392   0.92783976  0.9278398   0.9278405\n",
      "  0.9278407   0.92784035  0.92784065  0.92784077  0.9278409   0.9278408\n",
      "  0.9278412   0.92784125  0.92784154  0.92784137  0.9278419   0.9278418\n",
      "  0.9278416   0.927842    0.92784196  0.9278421   0.9278422   0.9278422\n",
      "  0.927842    0.92784274  0.92784226  0.92784274  0.927843    0.9278425\n",
      "  0.92784274  0.92784244  0.9278431   0.92784286  0.927843    0.92784315\n",
      "  0.9278426   0.9278426   0.92784303  0.9278434   0.9278431   0.9278429\n",
      "  0.9278429   0.9278431   0.9278431   0.92784303  0.92784303  0.92784303\n",
      "  0.92784303  0.9278432   0.9278433   0.92784303  0.92784345  0.92784286\n",
      "  0.92784303  0.9278425   0.9278428   0.92784274  0.9278428   0.9278429\n",
      "  0.92784274  0.92784226  0.92784214  0.92784244  0.92784226  0.92784226\n",
      "  0.9278421   0.92784214  0.9278418   0.9278419   0.927842    0.9278417\n",
      "  0.92784125  0.9278416   0.9278414   0.9278411   0.92784125  0.92784137\n",
      "  0.92784107  0.9278409   0.92784035  0.92784065  0.9278404   0.9278404\n",
      "  0.92784035  0.9278402   0.9278401   0.9278394   0.9278395   0.9278392\n",
      "  0.92783916  0.9278394   0.9278387   0.92783856  0.92783844  0.9278387\n",
      "  0.92783844  0.92783785  0.92783797  0.9278373   0.9278373   0.9278367\n",
      "  0.927837    0.927837    0.9278367   0.927836    0.92783624  0.9278359\n",
      "  0.9278359   0.9278356   0.92783517  0.9278354   0.9278348   0.9278344\n",
      "  0.9278341   0.92783403  0.9278337   0.92783374  0.9278333   0.9278331\n",
      "  0.92783266  0.92783254  0.9278323   0.9278318   0.927832    0.9278311\n",
      "  0.92783105  0.92783076  0.9278302   0.9278301   0.92783     0.92782974\n",
      "  0.92782897  0.9278289   0.9278286   0.92782825  0.9278281   0.9278274\n",
      "  0.9278271   0.92782706  0.9278266   0.9278268   0.92782617  0.92782575\n",
      "  0.9278251   0.9278251   0.92782503  0.92782456  0.9278235   0.9278235\n",
      "  0.9278233   0.92782265  0.9278227   0.92782205  0.9278218   0.9278213\n",
      "  0.927821    0.9278207   0.92782027  0.92781985  0.9278198   0.927819\n",
      "  0.92781866  0.92781836  0.9278182   0.9278174   0.9278174   0.9278167\n",
      "  0.92781615  0.9278156   0.92781526  0.9278146   0.9278143   0.927814\n",
      "  0.92781377  0.9278131   0.92781264  0.9278123   0.92781174  0.9278115\n",
      "  0.9278113   0.9278108   0.9278098   0.927809    0.9278087   0.92780817\n",
      "  0.9278083   0.927808    0.927807    0.92780614  0.92780614  0.9278055\n",
      "  0.9278049   0.9278047   0.927804    0.92780375  0.92780316  0.9278029\n",
      "  0.9278021   0.92780197  0.92780113  0.9278003   0.92780036  0.92779976\n",
      "  0.92779887  0.92779875  0.9277978   0.92779756  0.9277966   0.92779607\n",
      "  0.92779577  0.9277951   0.9277944   0.927794    0.9277938   0.9277929\n",
      "  0.9277927   0.9277917   0.9277909   0.9277904   0.9277902   0.9277894\n",
      "  0.9277889   0.9277884   0.92778784  0.9277871   0.9277864   0.9277857\n",
      "  0.9277857   0.9277848   0.9277842   0.9277839   0.9277827   0.92778206\n",
      "  0.92778146  0.92778134  0.9277803   0.9277801   0.9277791   0.9277781\n",
      "  0.9277778   0.92777735  0.92777663  0.92777616  0.9277755   0.927775\n",
      "  0.927774    0.9277735   0.9277727   0.92777216  0.92777133  0.9277706\n",
      "  0.92777014  0.9277693   0.92776865  0.9277682   0.92776734  0.9277665\n",
      "  0.9277658   0.92776537  0.92776483  0.92776376  0.92776334  0.92776275\n",
      "  0.92776155  0.9277613   0.9277604   0.92775995  0.92775923  0.9277584\n",
      "  0.92775756  0.9277569   0.92775613  0.9277557   0.92775476  0.9277541\n",
      "  0.9277535   0.92775273  0.9277517   0.92775047  0.92775005  0.9277494\n",
      "  0.92774886  0.92774856  0.9277473   0.9277468   0.9277456   0.9277451\n",
      "  0.9277441   0.927743    0.92774284  0.9277422   0.9277411   0.92774034\n",
      "  0.9277394   0.9277389   0.927738    0.9277372   0.9277364   0.9277353\n",
      "  0.92773515  0.9277341   0.9277336   0.9277326   0.9277319   0.9277307\n",
      "  0.9277299   0.92772895  0.9277287   0.92772734  0.9277266   0.92772615\n",
      "  0.9277251   0.9277238   0.9277234   0.9277228   0.9277218   0.9277212\n",
      "  0.92771983  0.927719    0.9277184   0.92771745  0.92771673  0.92771614\n",
      "  0.9277151   0.92771417  0.927713    0.9277126   0.9277114   0.9277106\n",
      "  0.9277097   0.9277088   0.92770797  0.92770684  0.9277061   0.92770517\n",
      "  0.92770463  0.927704    0.9277026   0.9277019   0.9277006   0.9277\n",
      "  0.9276992   0.92769825  0.92769736  0.9276961   0.9276956   0.927695\n",
      "  0.9276936   0.92769253  0.9276915   0.9276911   0.92768973  0.92768866\n",
      "  0.9276879   0.9276871   0.92768604  0.92768544  0.9276842   0.92768353\n",
      "  0.9276824   0.9276817   0.92768073  0.9276796   0.9276789   0.9276776\n",
      "  0.9276769   0.9276756   0.9276747   0.92767394  0.9276731   0.9276722\n",
      "  0.92767084  0.9276701   0.9276691   0.92766804  0.9276671   0.927666\n",
      "  0.9276646   0.9276642   0.92766315  0.9276618   0.92766106  0.92766005\n",
      "  0.9276588   0.9276579   0.92765707  0.927656    0.9276545   0.92765385\n",
      "  0.927653    0.9276519   0.9276513   0.92764986  0.9276489   0.9276478\n",
      "  0.92764693  0.92764527  0.9276445   0.9276435   0.9276427   0.92764163\n",
      "  0.92764056  0.92763984  0.9276385   0.92763764  0.9276361   0.92763495\n",
      "  0.9276344   0.92763335  0.92763233  0.92763096  0.9276298   0.92762923\n",
      "  0.9276282   0.9276267   0.9276256   0.92762506  0.9276238   0.9276226\n",
      "  0.92762125  0.9276204   0.92761886  0.92761844  0.9276168   0.9276157\n",
      "  0.9276149   0.9276138   0.9276129   0.92761135  0.9276106   0.92760944\n",
      "  0.9276085   0.9276074   0.9276061   0.92760456  0.92760384  0.92760277\n",
      "  0.92760164  0.92760074  0.9275992   0.9275981   0.9275971   0.9275959\n",
      "  0.9275948   0.9275938   0.9275924   0.9275913   0.92759037  0.9275894\n",
      "  0.9275878   0.9275871   0.9275854   0.9275843   0.92758346  0.9275816\n",
      "  0.92758095  0.9275799   0.9275787   0.9275777   0.92757666  0.92757547\n",
      "  0.9275742   0.9275726   0.9275715   0.92757034  0.9275693   0.92756826\n",
      "  0.92756706  0.9275656   0.9275646   0.9275636   0.9275623   0.92756104\n",
      "  0.92756003  0.9275587   0.92755705  0.9275562   0.927555    0.92755383\n",
      "  0.92755276  0.92755175  0.92755026  0.927549    0.9275479   0.92754656\n",
      "  0.9275451   0.9275438   0.9275431   0.9275419   0.92754066  0.9275392\n",
      "  0.9275378   0.9275365   0.9275355   0.92753404  0.92753315  0.9275316\n",
      "  0.9275307   0.92752916  0.9275281   0.927527    0.9275258   0.9275241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.92752314  0.92752165  0.92752045  0.9275195 ]\n",
      "[753.4099, 1000]\n"
     ]
    }
   ],
   "source": [
    "model_file_path = \"/tmp/es_2723/14h_35m_14s\"\n",
    "save_directory = \"/home/jovyan/work/evolution-strategies\"\n",
    "\n",
    "\n",
    "with Pool(os.cpu_count()) as pool:\n",
    "    pool.apply(func=run_model, args=(model_file_path, \"snapshot_01244.h5\", save_directory, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openaigym.video.0.24331.video000000.mp4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c107709d40441a1945d22bf731fe4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00\\x03\\x15gmdat\\x00\\xâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file in os.listdir(save_directory):\n",
    "    if file.endswith('.mp4'):\n",
    "        video_file = os.path.join(save_directory, file)\n",
    "        print(file)\n",
    "video = Video.from_file(video_file)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

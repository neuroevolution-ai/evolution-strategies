{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "tf util"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf # pylint: ignore-module\n",
    "import builtins\n",
    "import functools\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# ================================================================\n",
    "# Import all names into common namespace\n",
    "# ================================================================\n",
    "\n",
    "clip = tf.clip_by_value\n",
    "\n",
    "# Make consistent with numpy\n",
    "# ----------------------------------------\n",
    "\n",
    "def sum(x, axis=None, keepdims=False):\n",
    "    return tf.reduce_sum(x, reduction_indices=None if axis is None else [axis], keep_dims = keepdims)\n",
    "def mean(x, axis=None, keepdims=False):\n",
    "    return tf.reduce_mean(x, reduction_indices=None if axis is None else [axis], keep_dims = keepdims)\n",
    "def var(x, axis=None, keepdims=False):\n",
    "    meanx = mean(x, axis=axis, keepdims=keepdims)\n",
    "    return mean(tf.square(x - meanx), axis=axis, keepdims=keepdims)\n",
    "def std(x, axis=None, keepdims=False):\n",
    "    return tf.sqrt(var(x, axis=axis, keepdims=keepdims))\n",
    "def max(x, axis=None, keepdims=False):\n",
    "    return tf.reduce_max(x, reduction_indices=None if axis is None else [axis], keep_dims = keepdims)\n",
    "def min(x, axis=None, keepdims=False):\n",
    "    return tf.reduce_min(x, reduction_indices=None if axis is None else [axis], keep_dims = keepdims)\n",
    "def concatenate(arrs, axis=0):\n",
    "    return tf.concat(axis, arrs)\n",
    "def argmax(x, axis=None):\n",
    "    return tf.argmax(x, dimension=axis)\n",
    "\n",
    "def switch(condition, then_expression, else_expression):\n",
    "    '''Switches between two operations depending on a scalar value (int or bool).\n",
    "    Note that both `then_expression` and `else_expression`\n",
    "    should be symbolic tensors of the *same shape*.\n",
    "\n",
    "    # Arguments\n",
    "        condition: scalar tensor.\n",
    "        then_expression: TensorFlow operation.\n",
    "        else_expression: TensorFlow operation.\n",
    "    '''\n",
    "    x_shape = copy.copy(then_expression.get_shape())\n",
    "    x = tf.cond(tf.cast(condition, 'bool'),\n",
    "                lambda: then_expression,\n",
    "                lambda: else_expression)\n",
    "    x.set_shape(x_shape)\n",
    "    return x\n",
    "\n",
    "# Extras\n",
    "# ----------------------------------------\n",
    "def l2loss(params):\n",
    "    if len(params) == 0:\n",
    "        return tf.constant(0.0)\n",
    "    else:\n",
    "        return tf.add_n([sum(tf.square(p)) for p in params])\n",
    "def lrelu(x, leak=0.2):\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * x + f2 * abs(x)\n",
    "def categorical_sample_logits(X):\n",
    "    # https://github.com/tensorflow/tensorflow/issues/456\n",
    "    U = tf.random_uniform(tf.shape(X))\n",
    "    return argmax(X - tf.log(-tf.log(U)), axis=1)\n",
    "\n",
    "# ================================================================\n",
    "# Global session\n",
    "# ================================================================\n",
    "\n",
    "def get_session():\n",
    "    return tf.get_default_session()\n",
    "\n",
    "def single_threaded_session():\n",
    "    tf_config = tf.ConfigProto(\n",
    "        inter_op_parallelism_threads=1,\n",
    "        intra_op_parallelism_threads=1)\n",
    "    return tf.Session(config=tf_config)\n",
    "\n",
    "ALREADY_INITIALIZED = set()\n",
    "def initialize():\n",
    "    new_variables = set(tf.all_variables()) - ALREADY_INITIALIZED\n",
    "    get_session().run(tf.initialize_variables(new_variables))\n",
    "    ALREADY_INITIALIZED.update(new_variables)\n",
    "\n",
    "\n",
    "def eval(expr, feed_dict=None):\n",
    "    if feed_dict is None: feed_dict = {}\n",
    "    return get_session().run(expr, feed_dict=feed_dict)\n",
    "\n",
    "def set_value(v, val):\n",
    "    get_session().run(v.assign(val))\n",
    "\n",
    "def load_state(fname):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(get_session(), fname)\n",
    "\n",
    "def save_state(fname):\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(get_session(), fname)\n",
    "\n",
    "# ================================================================\n",
    "# Model components\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def normc_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None): #pylint: disable=W0613\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def dense(x, size, name, weight_init=None, bias=True):\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=weight_init)\n",
    "    ret = tf.matmul(x, w)\n",
    "    if bias:\n",
    "        b = tf.get_variable(name + \"/b\", [size], initializer=tf.zeros_initializer)\n",
    "        return ret + b\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "# ================================================================\n",
    "# Basic Stuff\n",
    "# ================================================================\n",
    "\n",
    "def function(inputs, outputs, updates=None, givens=None):\n",
    "    if isinstance(outputs, list):\n",
    "        return _Function(inputs, outputs, updates, givens=givens)\n",
    "    elif isinstance(outputs, dict):\n",
    "        f = _Function(inputs, outputs.values(), updates, givens=givens)\n",
    "        return lambda *inputs : dict(zip(outputs.keys(), f(*inputs)))\n",
    "    else:\n",
    "        f = _Function(inputs, [outputs], updates, givens=givens)\n",
    "        return lambda *inputs : f(*inputs)[0]\n",
    "\n",
    "class _Function(object):\n",
    "    def __init__(self, inputs, outputs, updates, givens, check_nan=False):\n",
    "        assert all(len(i.op.inputs)==0 for i in inputs), \"inputs should all be placeholders\"\n",
    "        self.inputs = inputs\n",
    "        updates = updates or []\n",
    "        self.update_group = tf.group(*updates)\n",
    "        self.outputs_update = list(outputs) + [self.update_group]\n",
    "        self.givens = {} if givens is None else givens\n",
    "        self.check_nan = check_nan\n",
    "    def __call__(self, *inputvals):\n",
    "        assert len(inputvals) == len(self.inputs)\n",
    "        feed_dict = dict(zip(self.inputs, inputvals))\n",
    "        feed_dict.update(self.givens)\n",
    "        results = get_session().run(self.outputs_update, feed_dict=feed_dict)[:-1]\n",
    "        if self.check_nan:\n",
    "            if any(np.isnan(r).any() for r in results):\n",
    "                raise RuntimeError(\"Nan detected\")\n",
    "        return results\n",
    "\n",
    "# ================================================================\n",
    "# Graph traversal\n",
    "# ================================================================\n",
    "\n",
    "VARIABLES = {}\n",
    "\n",
    "# ================================================================\n",
    "# Flat vectors\n",
    "# ================================================================\n",
    "\n",
    "def var_shape(x):\n",
    "    out = [k.value for k in x.get_shape()]\n",
    "    assert all(isinstance(a, int) for a in out), \\\n",
    "        \"shape function assumes that shape is fully known\"\n",
    "    return out\n",
    "\n",
    "def numel(x):\n",
    "    return intprod(var_shape(x))\n",
    "\n",
    "def intprod(x):\n",
    "    return int(np.prod(x))\n",
    "\n",
    "def flatgrad(loss, var_list):\n",
    "    grads = tf.gradients(loss, var_list)\n",
    "    return tf.concat(0, [tf.reshape(grad, [numel(v)])\n",
    "        for (v, grad) in zip(var_list, grads)])\n",
    "\n",
    "class SetFromFlat(object):\n",
    "    def __init__(self, var_list, dtype=tf.float32):\n",
    "        assigns = []\n",
    "        shapes = list(map(var_shape, var_list))\n",
    "        total_size = np.sum([intprod(shape) for shape in shapes])\n",
    "\n",
    "        self.theta = theta = tf.placeholder(dtype,[total_size])\n",
    "        start=0\n",
    "        assigns = []\n",
    "        for (shape,v) in zip(shapes,var_list):\n",
    "            size = intprod(shape)\n",
    "            assigns.append(tf.assign(v, tf.reshape(theta[start:start+size],shape)))\n",
    "            start+=size\n",
    "        assert start == total_size\n",
    "        self.op = tf.group(*assigns)\n",
    "    def __call__(self, theta):\n",
    "        get_session().run(self.op, feed_dict={self.theta:theta})\n",
    "\n",
    "class GetFlat(object):\n",
    "    def __init__(self, var_list):\n",
    "        x = [tf.reshape(v, [numel(v)]) for v in var_list]\n",
    "        self.op = tf.concat(x, 0) #Seems like defining axis as first parameter is deprecated, so axis as second parameter\n",
    "    def __call__(self):\n",
    "        return get_session().run(self.op)\n",
    "\n",
    "# ================================================================\n",
    "# Misc\n",
    "# ================================================================\n",
    "\n",
    "def scope_vars(scope, trainable_only):\n",
    "    \"\"\"\n",
    "    Get variables inside a scope\n",
    "    The scope can be specified as a string\n",
    "    \"\"\"\n",
    "    return tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES if trainable_only else tf.GraphKeys.VARIABLES,\n",
    "        scope=scope if isinstance(scope, str) else scope.name\n",
    "    )\n",
    "\n",
    "def in_session(f):\n",
    "    @functools.wraps(f)\n",
    "    def newfunc(*args, **kwargs):\n",
    "        with tf.Session():\n",
    "            f(*args, **kwargs)\n",
    "    return newfunc\n",
    "\n",
    "\n",
    "_PLACEHOLDER_CACHE = {} # name -> (placeholder, dtype, shape)\n",
    "def get_placeholder(name, dtype, shape):\n",
    "    print(\"calling get_placeholder\", name)\n",
    "    if name in _PLACEHOLDER_CACHE:\n",
    "        out, dtype1, shape1 = _PLACEHOLDER_CACHE[name]\n",
    "        assert dtype1==dtype and shape1==shape\n",
    "        return out\n",
    "    else:\n",
    "        out = tf.placeholder(dtype=dtype, shape=shape, name=name)\n",
    "        _PLACEHOLDER_CACHE[name] = (out,dtype,shape)\n",
    "        return out\n",
    "def get_placeholder_cached(name):\n",
    "    return _PLACEHOLDER_CACHE[name][0]\n",
    "\n",
    "def flattenallbut0(x):\n",
    "    return tf.reshape(x, [-1, intprod(x.get_shape().as_list()[1:])])\n",
    "\n",
    "def reset():\n",
    "    global _PLACEHOLDER_CACHE\n",
    "    global VARIABLES\n",
    "    _PLACEHOLDER_CACHE = {}\n",
    "    VARIABLES = {}\n",
    "    tf.reset_default_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tabular logger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.python import pywrap_tensorflow\n",
    "from tensorflow.python.util import compat\n",
    "\n",
    "DEBUG = 10\n",
    "INFO = 20\n",
    "WARN = 30\n",
    "ERROR = 40\n",
    "\n",
    "DISABLED = 50\n",
    "\n",
    "class TbWriter(object):\n",
    "    \"\"\"\n",
    "    Based on SummaryWriter, but changed to allow for a different prefix\n",
    "    and to get rid of multithreading\n",
    "    oops, ended up using the same prefix anyway.\n",
    "    \"\"\"\n",
    "    def __init__(self, dir, prefix):\n",
    "        self.dir = dir\n",
    "        self.step = 1 # Start at 1, because EvWriter automatically generates an object with step=0\n",
    "        self.evwriter = pywrap_tensorflow.EventsWriter(compat.as_bytes(os.path.join(dir, prefix)))\n",
    "    def write_values(self, key2val):\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=k, simple_value=float(v))\n",
    "            for (k, v) in key2val.items()])\n",
    "        event = event_pb2.Event(wall_time=time.time(), summary=summary)\n",
    "        event.step = self.step # is there any reason why you'd want to specify the step?\n",
    "        self.evwriter.WriteEvent(event)\n",
    "        self.evwriter.Flush()\n",
    "        self.step += 1\n",
    "    def close(self):\n",
    "        self.evwriter.Close()\n",
    "\n",
    "# ================================================================\n",
    "# API \n",
    "# ================================================================\n",
    "\n",
    "def start(dir):\n",
    "    \"\"\"\n",
    "    dir: directory to put all output files\n",
    "    force: if dir already exists, should we delete it, or throw a RuntimeError?\n",
    "    \"\"\"\n",
    "    if _Logger.CURRENT is not _Logger.DEFAULT:\n",
    "        sys.stderr.write(\"WARNING: You asked to start logging (dir=%s), but you never stopped the previous logger (dir=%s).\\n\"%(dir, _Logger.CURRENT.dir))\n",
    "    _Logger.CURRENT = _Logger(dir=dir)\n",
    "\n",
    "def stop():\n",
    "    if _Logger.CURRENT is _Logger.DEFAULT:\n",
    "        sys.stderr.write(\"WARNING: You asked to stop logging, but you never started any previous logger.\\n\"%(dir, _Logger.CURRENT.dir))\n",
    "        return\n",
    "    _Logger.CURRENT.close()\n",
    "    _Logger.CURRENT = _Logger.DEFAULT\n",
    "\n",
    "def record_tabular(key, val):\n",
    "    \"\"\"\n",
    "    Log a value of some diagnostic\n",
    "    Call this once for each diagnostic quantity, each iteration\n",
    "    \"\"\"\n",
    "    _Logger.CURRENT.record_tabular(key, val)\n",
    "\n",
    "def dump_tabular():\n",
    "    \"\"\"\n",
    "    Write all of the diagnostics from the current iteration\n",
    "\n",
    "    level: int. (see logger.py docs) If the global logger level is higher than\n",
    "                the level argument here, don't print to stdout.\n",
    "    \"\"\"\n",
    "    _Logger.CURRENT.dump_tabular()\n",
    "\n",
    "def log(*args, level=INFO):\n",
    "    \"\"\"\n",
    "    Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).\n",
    "    \"\"\"\n",
    "    _Logger.CURRENT.log(*args, level=level)\n",
    "\n",
    "def debug(*args):\n",
    "    log(*args, level=DEBUG)\n",
    "def info(*args):\n",
    "    log(*args, level=INFO)\n",
    "def warn(*args):\n",
    "    log(*args, level=WARN)\n",
    "def error(*args):\n",
    "    log(*args, level=ERROR)\n",
    "\n",
    "def set_level(level):\n",
    "    \"\"\"\n",
    "    Set logging threshold on current logger.\n",
    "    \"\"\"\n",
    "    _Logger.CURRENT.set_level(level)\n",
    "\n",
    "def get_dir():\n",
    "    \"\"\"\n",
    "    Get directory that log files are being written to.\n",
    "    will be None if there is no output directory (i.e., if you didn't call start)\n",
    "    \"\"\"\n",
    "    return _Logger.CURRENT.get_dir()\n",
    "\n",
    "def get_expt_dir():\n",
    "    sys.stderr.write(\"get_expt_dir() is Deprecated. Switch to get_dir()\\n\")\n",
    "    return get_dir()\n",
    "\n",
    "# ================================================================\n",
    "# Backend \n",
    "# ================================================================\n",
    "\n",
    "class _Logger(object):\n",
    "    DEFAULT = None # A logger with no output files. (See right below class definition) \n",
    "                   # So that you can still log to the terminal without setting up any output files\n",
    "    CURRENT = None # Current logger being used by the free functions above\n",
    "\n",
    "    def __init__(self, dir=None):\n",
    "        self.name2val = OrderedDict() # values this iteration\n",
    "        self.level = INFO\n",
    "        self.dir = dir\n",
    "        self.text_outputs = [sys.stdout]\n",
    "        if dir is not None:\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "            self.text_outputs.append(open(os.path.join(dir, \"log.txt\"), \"w\"))\n",
    "            self.tbwriter = TbWriter(dir=dir, prefix=\"events\")\n",
    "        else:\n",
    "            self.tbwriter = None\n",
    "\n",
    "    # Logging API, forwarded\n",
    "    # ----------------------------------------\n",
    "    def record_tabular(self, key, val):\n",
    "        self.name2val[key] = val\n",
    "    def dump_tabular(self):\n",
    "        # Create strings for printing\n",
    "        key2str = OrderedDict()\n",
    "        for (key,val) in self.name2val.items():\n",
    "            if hasattr(val, \"__float__\"): valstr = \"%-8.3g\"%val\n",
    "            else: valstr = val\n",
    "            key2str[self._truncate(key)]=self._truncate(valstr)\n",
    "        keywidth = max(map(len, key2str.keys()))\n",
    "        valwidth = max(map(len, key2str.values()))\n",
    "        # Write to all text outputs\n",
    "        self._write_text(\"-\"*(keywidth+valwidth+7), \"\\n\")\n",
    "        for (key,val) in key2str.items():\n",
    "            self._write_text(\"| \", key, \" \"*(keywidth-len(key)), \" | \", val, \" \"*(valwidth-len(val)), \" |\\n\")\n",
    "        self._write_text(\"-\"*(keywidth+valwidth+7), \"\\n\")\n",
    "        for f in self.text_outputs:\n",
    "            try: f.flush()\n",
    "            except OSError: sys.stderr.write('Warning! OSError when flushing.\\n')\n",
    "        # Write to tensorboard\n",
    "        if self.tbwriter is not None:\n",
    "            self.tbwriter.write_values(self.name2val)\n",
    "            self.name2val.clear()\n",
    "    def log(self, *args, level=INFO):\n",
    "        if self.level <= level:\n",
    "            self._do_log(*args)\n",
    "\n",
    "    # Configuration \n",
    "    # ----------------------------------------\n",
    "    def set_level(self, level):\n",
    "        self.level = level\n",
    "    def get_dir(self):\n",
    "        return self.dir\n",
    "\n",
    "    def close(self):\n",
    "        for f in self.text_outputs[1:]: f.close()\n",
    "        if self.tbwriter: self.tbwriter.close()\n",
    "\n",
    "    # Misc \n",
    "    # ----------------------------------------\n",
    "    def _do_log(self, *args):\n",
    "        self._write_text(*args, '\\n')\n",
    "        for f in self.text_outputs:\n",
    "            try: f.flush()\n",
    "            except OSError: print('Warning! OSError when flushing.')                \n",
    "    def _write_text(self, *strings):\n",
    "        for f in self.text_outputs:\n",
    "            for string in strings:\n",
    "                f.write(string)\n",
    "    def _truncate(self, s):\n",
    "        if len(s) > 33:\n",
    "            return s[:30] + \"...\"\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "_Logger.DEFAULT = _Logger()\n",
    "_Logger.CURRENT = _Logger.DEFAULT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _demo():\n",
    "    info(\"hi\")\n",
    "    debug(\"shouldn't appear\")\n",
    "    set_level(DEBUG)\n",
    "    debug(\"should appear\")\n",
    "    dir = \"/tmp/testlogging\"\n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir)\n",
    "    start(dir=dir)\n",
    "    record_tabular(\"a\", 3)\n",
    "    record_tabular(\"b\", 2.5)\n",
    "    dump_tabular()\n",
    "    record_tabular(\"b\", -2.5)\n",
    "    record_tabular(\"a\", 5.5)\n",
    "    dump_tabular()\n",
    "    info(\"^^^ should see a = 5.5\")\n",
    "    stop()\n",
    "\n",
    "    try:\n",
    "        record_tabular(\"newthing\", 5.5)\n",
    "    except AssertionError:\n",
    "        pass\n",
    "\n",
    "    record_tabular(\"b\", -2.5)\n",
    "    dump_tabular()\n",
    "\n",
    "\n",
    "    record_tabular(\"a\", \"asdfasdfasdf\")\n",
    "    dump_tabular()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _demo()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "policies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import tf_util as U\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.args, self.kwargs = args, kwargs\n",
    "        self.scope = self._initialize(*args, **kwargs)\n",
    "        self.all_variables = tf.get_collection(tf.GraphKeys.VARIABLES, self.scope.name)\n",
    "\n",
    "        self.trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope.name)\n",
    "        self.num_params = sum(int(np.prod(v.get_shape().as_list())) for v in self.trainable_variables)\n",
    "        self._setfromflat = U.SetFromFlat(self.trainable_variables)\n",
    "        self._getflat = U.GetFlat(self.trainable_variables)\n",
    "\n",
    "        logger.info('Trainable variables ({} parameters)'.format(self.num_params))\n",
    "        for v in self.trainable_variables:\n",
    "            shp = v.get_shape().as_list()\n",
    "            logger.info('- {} shape:{} size:{}'.format(v.name, shp, np.prod(shp)))\n",
    "        logger.info('All variables')\n",
    "        for v in self.all_variables:\n",
    "            shp = v.get_shape().as_list()\n",
    "            logger.info('- {} shape:{} size:{}'.format(v.name, shp, np.prod(shp)))\n",
    "\n",
    "        placeholders = [tf.placeholder(v.value().dtype, v.get_shape().as_list()) for v in self.all_variables]\n",
    "        self.set_all_vars = U.function(\n",
    "            inputs=placeholders,\n",
    "            outputs=[],\n",
    "            updates=[tf.group(*[v.assign(p) for v, p in zip(self.all_variables, placeholders)])]\n",
    "        )\n",
    "\n",
    "    def _initialize(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, filename):\n",
    "        assert filename.endswith('.h5')\n",
    "        with h5py.File(filename, 'w') as f:\n",
    "            for v in self.all_variables:\n",
    "                f[v.name] = v.eval()\n",
    "            # TODO: it would be nice to avoid pickle, but it's convenient to pass Python objects to _initialize\n",
    "            # (like Gym spaces or numpy arrays)\n",
    "            f.attrs['name'] = type(self).__name__\n",
    "            f.attrs['args_and_kwargs'] = np.void(pickle.dumps((self.args, self.kwargs), protocol=-1))\n",
    "\n",
    "    @classmethod\n",
    "    def Load(cls, filename, extra_kwargs=None):\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            args, kwargs = pickle.loads(f.attrs['args_and_kwargs'].tostring())\n",
    "            if extra_kwargs:\n",
    "                kwargs.update(extra_kwargs)\n",
    "            policy = cls(*args, **kwargs)\n",
    "            policy.set_all_vars(*[f[v.name][...] for v in policy.all_variables])\n",
    "        return policy\n",
    "\n",
    "    # === Rollouts/training ===\n",
    "\n",
    "    def rollout(self, env, *, render=False, timestep_limit=None, save_obs=False, random_stream=None):\n",
    "        \"\"\"\n",
    "        If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "        Otherwise, no action noise will be added.\n",
    "        \"\"\"\n",
    "        env_timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "        timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "        rews = []\n",
    "        t = 0\n",
    "        if save_obs:\n",
    "            obs = []\n",
    "        ob = env.reset()\n",
    "        for _ in range(timestep_limit):\n",
    "            ac = self.act(ob[None], random_stream=random_stream)[0]\n",
    "            if save_obs:\n",
    "                obs.append(ob)\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "            rews.append(rew)\n",
    "            t += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        rews = np.array(rews, dtype=np.float32)\n",
    "        if save_obs:\n",
    "            return rews, t, np.array(obs)\n",
    "        return rews, t\n",
    "\n",
    "    def act(self, ob, random_stream=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_trainable_flat(self, x):\n",
    "        self._setfromflat(x)\n",
    "\n",
    "    def get_trainable_flat(self):\n",
    "        return self._getflat()\n",
    "\n",
    "    @property\n",
    "    def needs_ob_stat(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_ob_stat(self, ob_mean, ob_std):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def bins(x, dim, num_bins, name):\n",
    "    scores = U.dense(x, dim * num_bins, name, U.normc_initializer(0.01))\n",
    "    scores_nab = tf.reshape(scores, [-1, dim, num_bins])\n",
    "    return tf.argmax(scores_nab, 2)  # 0 ... num_bins-1\n",
    "\n",
    "\n",
    "class MujocoPolicy(Policy):\n",
    "    def _initialize(self, ob_space, ac_space, ac_bins, ac_noise_std, nonlin_type, hidden_dims, connection_type):\n",
    "        self.ac_space = ac_space\n",
    "        self.ac_bins = ac_bins\n",
    "        self.ac_noise_std = ac_noise_std\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.connection_type = connection_type\n",
    "\n",
    "        assert len(ob_space.shape) == len(self.ac_space.shape) == 1\n",
    "        assert np.all(np.isfinite(self.ac_space.low)) and np.all(np.isfinite(self.ac_space.high)), \\\n",
    "            'Action bounds required'\n",
    "\n",
    "        self.nonlin = {'tanh': tf.tanh, 'relu': tf.nn.relu, 'lrelu': U.lrelu, 'elu': tf.nn.elu}[nonlin_type]\n",
    "\n",
    "        with tf.variable_scope(type(self).__name__) as scope:\n",
    "            # Observation normalization\n",
    "            ob_mean = tf.get_variable(\n",
    "                'ob_mean', ob_space.shape, tf.float32, tf.constant_initializer(np.nan), trainable=False)\n",
    "            ob_std = tf.get_variable(\n",
    "                'ob_std', ob_space.shape, tf.float32, tf.constant_initializer(np.nan), trainable=False)\n",
    "            in_mean = tf.placeholder(tf.float32, ob_space.shape)\n",
    "            in_std = tf.placeholder(tf.float32, ob_space.shape)\n",
    "            self._set_ob_mean_std = U.function([in_mean, in_std], [], updates=[\n",
    "                tf.assign(ob_mean, in_mean),\n",
    "                tf.assign(ob_std, in_std),\n",
    "            ])\n",
    "\n",
    "            # Policy network\n",
    "            o = tf.placeholder(tf.float32, [None] + list(ob_space.shape))\n",
    "            a = self._make_net(tf.clip_by_value((o - ob_mean) / ob_std, -5.0, 5.0))\n",
    "            self._act = U.function([o], a)\n",
    "        return scope\n",
    "\n",
    "    def _make_net(self, o):\n",
    "        # Process observation\n",
    "        if self.connection_type == 'ff':\n",
    "            x = o\n",
    "            for ilayer, hd in enumerate(self.hidden_dims):\n",
    "                x = self.nonlin(U.dense(x, hd, 'l{}'.format(ilayer), U.normc_initializer(1.0)))\n",
    "        else:\n",
    "            raise NotImplementedError(self.connection_type)\n",
    "\n",
    "        # Map to action\n",
    "        adim, ahigh, alow = self.ac_space.shape[0], self.ac_space.high, self.ac_space.low\n",
    "        assert isinstance(self.ac_bins, str)\n",
    "        ac_bin_mode, ac_bin_arg = self.ac_bins.split(':')\n",
    "\n",
    "        if ac_bin_mode == 'uniform':\n",
    "            # Uniformly spaced bins, from ac_space.low to ac_space.high\n",
    "            num_ac_bins = int(ac_bin_arg)\n",
    "            aidx_na = bins(x, adim, num_ac_bins, 'out')  # 0 ... num_ac_bins-1\n",
    "            ac_range_1a = (ahigh - alow)[None, :]\n",
    "            a = 1. / (num_ac_bins - 1.) * tf.to_float(aidx_na) * ac_range_1a + alow[None, :]\n",
    "\n",
    "        elif ac_bin_mode == 'custom':\n",
    "            # Custom bins specified as a list of values from -1 to 1\n",
    "            # The bins are rescaled to ac_space.low to ac_space.high\n",
    "            acvals_k = np.array(list(map(float, ac_bin_arg.split(','))), dtype=np.float32)\n",
    "            logger.info('Custom action values: ' + ' '.join('{:.3f}'.format(x) for x in acvals_k))\n",
    "            assert acvals_k.ndim == 1 and acvals_k[0] == -1 and acvals_k[-1] == 1\n",
    "            acvals_ak = (\n",
    "                (ahigh - alow)[:, None] / (acvals_k[-1] - acvals_k[0]) * (acvals_k - acvals_k[0])[None, :]\n",
    "                + alow[:, None]\n",
    "            )\n",
    "\n",
    "            aidx_na = bins(x, adim, len(acvals_k), 'out')  # values in [0, k-1]\n",
    "            a = tf.gather_nd(\n",
    "                acvals_ak,\n",
    "                tf.concat(2, [\n",
    "                    tf.tile(np.arange(adim)[None, :, None], [tf.shape(aidx_na)[0], 1, 1]),\n",
    "                    tf.expand_dims(aidx_na, -1)\n",
    "                ])  # (n,a,2)\n",
    "            )  # (n,a)\n",
    "        elif ac_bin_mode == 'continuous':\n",
    "            a = U.dense(x, adim, 'out', U.normc_initializer(0.01))\n",
    "        else:\n",
    "            raise NotImplementedError(ac_bin_mode)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def act(self, ob, random_stream=None):\n",
    "        a = self._act(ob)\n",
    "        if random_stream is not None and self.ac_noise_std != 0:\n",
    "            a += random_stream.randn(*a.shape) * self.ac_noise_std\n",
    "        return a\n",
    "\n",
    "    @property\n",
    "    def needs_ob_stat(self):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def needs_ref_batch(self):\n",
    "        return False\n",
    "\n",
    "    def set_ob_stat(self, ob_mean, ob_std):\n",
    "        self._set_ob_mean_std(ob_mean, ob_std)\n",
    "\n",
    "    def initialize_from(self, filename, ob_stat=None):\n",
    "        \"\"\"\n",
    "        Initializes weights from another policy, which must have the same architecture (variable names),\n",
    "        but the weight arrays can be smaller than the current policy.\n",
    "        \"\"\"\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            f_var_names = []\n",
    "            f.visititems(lambda name, obj: f_var_names.append(name) if isinstance(obj, h5py.Dataset) else None)\n",
    "            assert set(v.name for v in self.all_variables) == set(f_var_names), 'Variable names do not match'\n",
    "\n",
    "            init_vals = []\n",
    "            for v in self.all_variables:\n",
    "                shp = v.get_shape().as_list()\n",
    "                f_shp = f[v.name].shape\n",
    "                assert len(shp) == len(f_shp) and all(a >= b for a, b in zip(shp, f_shp)), \\\n",
    "                    'This policy must have more weights than the policy to load'\n",
    "                init_val = v.eval()\n",
    "                # ob_mean and ob_std are initialized with nan, so set them manually\n",
    "                if 'ob_mean' in v.name:\n",
    "                    init_val[:] = 0\n",
    "                    init_mean = init_val\n",
    "                elif 'ob_std' in v.name:\n",
    "                    init_val[:] = 0.001\n",
    "                    init_std = init_val\n",
    "                # Fill in subarray from the loaded policy\n",
    "                init_val[tuple([np.s_[:s] for s in f_shp])] = f[v.name]\n",
    "                init_vals.append(init_val)\n",
    "            self.set_all_vars(*init_vals)\n",
    "\n",
    "        if ob_stat is not None:\n",
    "            ob_stat.set_from_init(init_mean, init_std, init_count=1e5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "optimizers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Optimizer(object):\n",
    "    def __init__(self, pi):\n",
    "        self.pi = pi\n",
    "        self.dim = pi.num_params\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, globalg):\n",
    "        self.t += 1\n",
    "        step = self._compute_step(globalg)\n",
    "        theta = self.pi.get_trainable_flat()\n",
    "        ratio = np.linalg.norm(step) / np.linalg.norm(theta)\n",
    "        self.pi.set_trainable_flat(theta + step)\n",
    "        return ratio\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, pi, stepsize, momentum=0.9):\n",
    "        Optimizer.__init__(self, pi)\n",
    "        self.v = np.zeros(self.dim, dtype=np.float32)\n",
    "        self.stepsize, self.momentum = stepsize, momentum\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        self.v = self.momentum * self.v + (1. - self.momentum) * globalg\n",
    "        step = -self.stepsize * self.v\n",
    "        return step\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, pi, stepsize, beta1=0.9, beta2=0.999, epsilon=1e-08):\n",
    "        Optimizer.__init__(self, pi)\n",
    "        self.stepsize = stepsize\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = np.zeros(self.dim, dtype=np.float32)\n",
    "        self.v = np.zeros(self.dim, dtype=np.float32)\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        a = self.stepsize * np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * globalg\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (globalg * globalg)\n",
    "        step = -a * self.m / (np.sqrt(self.v) + self.epsilon)\n",
    "        return step\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "es"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "Config = namedtuple('Config', [\n",
    "    'l2coeff', 'noise_stdev', 'episodes_per_batch', 'timesteps_per_batch',\n",
    "    'calc_obstat_prob', 'eval_prob', 'snapshot_freq',\n",
    "    'return_proc_mode', 'episode_cutoff_mode'\n",
    "])\n",
    "Task = namedtuple('Task', ['params', 'ob_mean', 'ob_std', 'timestep_limit', 'task_id'])\n",
    "Result = namedtuple('Result', [\n",
    "    'worker_id',\n",
    "    'noise_inds_n', 'returns_n2', 'signreturns_n2', 'lengths_n2',\n",
    "    'eval_return', 'eval_length',\n",
    "    'ob_sum', 'ob_sumsq', 'ob_count',\n",
    "    'task_id'\n",
    "])\n",
    "\n",
    "\n",
    "class RunningStat(object):\n",
    "    def __init__(self, shape, eps):\n",
    "        self.sum = np.zeros(shape, dtype=np.float32)\n",
    "        self.sumsq = np.full(shape, eps, dtype=np.float32)\n",
    "        self.count = eps\n",
    "\n",
    "    def increment(self, s, ssq, c):\n",
    "        self.sum += s\n",
    "        self.sumsq += ssq\n",
    "        self.count += c\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.sum / self.count\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(np.maximum(self.sumsq / self.count - np.square(self.mean), 1e-2))\n",
    "\n",
    "    def set_from_init(self, init_mean, init_std, init_count):\n",
    "        self.sum[:] = init_mean * init_count\n",
    "        self.sumsq[:] = (np.square(init_mean) + np.square(init_std)) * init_count\n",
    "        self.count = init_count\n",
    "\n",
    "\n",
    "class SharedNoiseTable(object):\n",
    "    def __init__(self):\n",
    "        import ctypes, multiprocessing\n",
    "        seed = 123\n",
    "        count = 250000000  # 1 gigabyte of 32-bit numbers. Will actually sample 2 gigabytes below.\n",
    "        logger.info('Sampling {} random numbers with seed {}'.format(count, seed))\n",
    "\n",
    "        # Instantiate an array of C float datatype with size count\n",
    "        self._shared_mem = multiprocessing.Array(ctypes.c_float, count)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        self.noise = np.ctypeslib.as_array(self._shared_mem.get_obj())\n",
    "        assert self.noise.dtype == np.float32\n",
    "        self.noise[:] = np.random.RandomState(seed).randn(count)  # 64-bit to 32-bit conversion here\n",
    "        logger.info('Sampled {} bytes'.format(self.noise.size * 4))\n",
    "\n",
    "    def get(self, i, dim):\n",
    "        return self.noise[i:i + dim]\n",
    "\n",
    "    def sample_index(self, stream, dim):\n",
    "        return stream.randint(0, len(self.noise) - dim + 1)\n",
    "\n",
    "\n",
    "def compute_ranks(x):\n",
    "    \"\"\"\n",
    "    Returns ranks in [0, len(x))\n",
    "    Note: This is different from scipy.stats.rankdata, which returns ranks in [1, len(x)].\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "    ranks = np.empty(len(x), dtype=int)\n",
    "    ranks[x.argsort()] = np.arange(len(x))\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def compute_centered_ranks(x):\n",
    "    y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "    y /= (x.size - 1)\n",
    "    y -= .5\n",
    "    return y\n",
    "\n",
    "\n",
    "def make_session(single_threaded):\n",
    "    \"\"\"\n",
    "    Start an InteractiveSession.\n",
    "\n",
    "    :param single_threaded: When True starts a Session without parameters, when False provides a ConfigProto object\n",
    "    :return: The InteractiveSession\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    if not single_threaded:\n",
    "        return tf.InteractiveSession()\n",
    "    return tf.InteractiveSession(config=tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1))\n",
    "\n",
    "\n",
    "def itergroups(items, group_size):\n",
    "    assert group_size >= 1\n",
    "    group = []\n",
    "    for x in items:\n",
    "        group.append(x)\n",
    "        if len(group) == group_size:\n",
    "            yield tuple(group)\n",
    "            del group[:]\n",
    "    if group:\n",
    "        yield tuple(group)\n",
    "\n",
    "\n",
    "def batched_weighted_sum(weights, vecs, batch_size):\n",
    "    total = 0.\n",
    "    num_items_summed = 0\n",
    "    for batch_weights, batch_vecs in zip(itergroups(weights, batch_size), itergroups(vecs, batch_size)):\n",
    "        assert len(batch_weights) == len(batch_vecs) <= batch_size\n",
    "        total += np.dot(np.asarray(batch_weights, dtype=np.float32), np.asarray(batch_vecs, dtype=np.float32))\n",
    "        num_items_summed += len(batch_weights)\n",
    "    return total, num_items_summed\n",
    "\n",
    "\n",
    "def setup(exp, single_threaded):\n",
    "    \"\"\"\n",
    "    Setup the environment\n",
    "\n",
    "    :param exp: Configuration JSON\n",
    "    :param single_threaded: todo\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    import gym\n",
    "\n",
    "    # Needed to register the roboschool environments within gym\n",
    "    import roboschool\n",
    "\n",
    "    from . import policies, tf_util\n",
    "\n",
    "    # import config from JSON\n",
    "    config = Config(**exp['config'])\n",
    "\n",
    "    env = gym.make(exp['env_id'])\n",
    "\n",
    "    # start tf session\n",
    "    sess = make_session(single_threaded=single_threaded)\n",
    "\n",
    "    # Instantiate an child object of the Policy Class as defined in \"type\" of the JSON file\n",
    "    policy = getattr(policies, exp['policy']['type'])(env.observation_space, env.action_space, **exp['policy']['args'])\n",
    "    tf_util.initialize()\n",
    "\n",
    "    return config, env, sess, policy\n",
    "\n",
    "def pop_item(queue, lock=None):\n",
    "    item = queue.get()\n",
    "\n",
    "    return item\n",
    "\n",
    "def get_task(task_queue, lock=None):\n",
    "    item = task_queue.get()\n",
    "\n",
    "    # todo find a better solution for this. Maybe multiprocessing array\n",
    "    task_queue.put(item)\n",
    "\n",
    "    return item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_master(exp, task_queue, result_queue, lock, log_dir):\n",
    "    logger.info('run_master: {}'.format(locals()))\n",
    "    from .optimizers import SGD, Adam\n",
    "    from . import tabular_logger as tlogger\n",
    "    logger.info('Tabular logging to {}'.format(log_dir))\n",
    "    tlogger.start(log_dir)\n",
    "\n",
    "    config, env, sess, policy = setup(exp, single_threaded=False)\n",
    "    optimizer = {'sgd': SGD, 'adam': Adam}[exp['optimizer']['type']](policy, **exp['optimizer']['args'])\n",
    "    noise = SharedNoiseTable()\n",
    "    rs = np.random.RandomState()\n",
    "    ob_stat = RunningStat(\n",
    "        env.observation_space.shape,\n",
    "        eps=1e-2  # eps to prevent dividing by zero at the beginning when computing mean/stdev\n",
    "    )\n",
    "\n",
    "    # Use existing weights from other policy\n",
    "    if 'init_from' in exp['policy']:\n",
    "        logger.info('Initializing weights from {}'.format(exp['policy']['init_from']))\n",
    "        policy.initialize_from(exp['policy']['init_from'], ob_stat)\n",
    "\n",
    "    # Check if config wants to cutoff an episode at some point\n",
    "    if config.episode_cutoff_mode.startswith('adaptive:'):\n",
    "        _, args = config.episode_cutoff_mode.split(':')\n",
    "        arg0, arg1, arg2 = args.split(',')\n",
    "        tslimit, incr_tslimit_threshold, tslimit_incr_ratio = int(arg0), float(arg1), float(arg2)\n",
    "        adaptive_tslimit = True\n",
    "        logger.info(\n",
    "            'Starting timestep limit set to {}. When {}% of rollouts hit the limit, it will be increased by {}'.format(\n",
    "                tslimit, incr_tslimit_threshold * 100, tslimit_incr_ratio))\n",
    "    elif config.episode_cutoff_mode == 'env_default':\n",
    "        tslimit, incr_tslimit_threshold, tslimit_incr_ratio = None, None, None\n",
    "        adaptive_tslimit = False\n",
    "    else:\n",
    "        raise NotImplementedError(config.episode_cutoff_mode)\n",
    "\n",
    "    episodes_so_far = 0\n",
    "    timesteps_so_far = 0\n",
    "    tstart = time.time()\n",
    "\n",
    "    task_counter = 0\n",
    "\n",
    "    while True:\n",
    "        step_tstart = time.time()\n",
    "        theta = policy.get_trainable_flat()\n",
    "        assert theta.dtype == np.float32\n",
    "\n",
    "        # Task counter is used to recognize false tasks from previous iterations later\n",
    "        curr_task_id = task_counter\n",
    "        task_counter += 1\n",
    "\n",
    "        task_queue.put(Task(\n",
    "            params=theta,\n",
    "            ob_mean=ob_stat.mean if policy.needs_ob_stat else None,\n",
    "            ob_std=ob_stat.std if policy.needs_ob_stat else None,\n",
    "            timestep_limit=tslimit,\n",
    "            task_id = curr_task_id\n",
    "        ))\n",
    "\n",
    "        tlogger.log('********** Iteration {} **********'.format(curr_task_id))\n",
    "\n",
    "        # Pop off results for the current task\n",
    "        curr_task_results, eval_rets, eval_lens, worker_ids = [], [], [], []\n",
    "        num_results_skipped, num_episodes_popped, num_timesteps_popped, ob_count_this_batch = 0, 0, 0, 0\n",
    "        while num_episodes_popped < config.episodes_per_batch or num_timesteps_popped < config.timesteps_per_batch:\n",
    "            # Wait for a result\n",
    "            result = pop_item(result_queue, lock)\n",
    "\n",
    "            assert isinstance(result, Result)\n",
    "            task_id = result.task_id\n",
    "            assert isinstance(task_id, int)\n",
    "\n",
    "            assert (result.eval_return is None) == (result.eval_length is None)\n",
    "            worker_ids.append(result.worker_id)\n",
    "\n",
    "            if result.eval_length is not None:\n",
    "                # This was an eval job\n",
    "                episodes_so_far += 1\n",
    "                timesteps_so_far += result.eval_length\n",
    "                # Store the result only for current tasks\n",
    "                if task_id == curr_task_id:\n",
    "                    eval_rets.append(result.eval_return)\n",
    "                    eval_lens.append(result.eval_length)\n",
    "            else:\n",
    "                # The real shit\n",
    "                assert (result.noise_inds_n.ndim == 1 and\n",
    "                        result.returns_n2.shape == result.lengths_n2.shape == (len(result.noise_inds_n), 2))\n",
    "                assert result.returns_n2.dtype == np.float32\n",
    "                # Update counts\n",
    "                result_num_eps = result.lengths_n2.size\n",
    "                result_num_timesteps = result.lengths_n2.sum()\n",
    "                episodes_so_far += result_num_eps\n",
    "                timesteps_so_far += result_num_timesteps\n",
    "                # Store results only for current tasks\n",
    "                if task_id == curr_task_id:\n",
    "                    curr_task_results.append(result)\n",
    "                    num_episodes_popped += result_num_eps\n",
    "                    num_timesteps_popped += result_num_timesteps\n",
    "                    # Update ob stats\n",
    "                    if policy.needs_ob_stat and result.ob_count > 0:\n",
    "                        ob_stat.increment(result.ob_sum, result.ob_sumsq, result.ob_count)\n",
    "                        ob_count_this_batch += result.ob_count\n",
    "                else:\n",
    "                    num_results_skipped += 1\n",
    "\n",
    "        # Compute skip fraction\n",
    "        frac_results_skipped = num_results_skipped / (num_results_skipped + len(curr_task_results))\n",
    "        if num_results_skipped > 0:\n",
    "            logger.warning('Skipped {} out of date results ({:.2f}%)'.format(\n",
    "                num_results_skipped, 100. * frac_results_skipped))\n",
    "\n",
    "        # Assemble results\n",
    "        noise_inds_n = np.concatenate([r.noise_inds_n for r in curr_task_results])\n",
    "        returns_n2 = np.concatenate([r.returns_n2 for r in curr_task_results])\n",
    "        lengths_n2 = np.concatenate([r.lengths_n2 for r in curr_task_results])\n",
    "        assert noise_inds_n.shape[0] == returns_n2.shape[0] == lengths_n2.shape[0]\n",
    "        # Process returns\n",
    "        if config.return_proc_mode == 'centered_rank':\n",
    "            proc_returns_n2 = compute_centered_ranks(returns_n2)\n",
    "        elif config.return_proc_mode == 'sign':\n",
    "            proc_returns_n2 = np.concatenate([r.signreturns_n2 for r in curr_task_results])\n",
    "        elif config.return_proc_mode == 'centered_sign_rank':\n",
    "            proc_returns_n2 = compute_centered_ranks(np.concatenate([r.signreturns_n2 for r in curr_task_results]))\n",
    "        else:\n",
    "            raise NotImplementedError(config.return_proc_mode)\n",
    "        # Compute and take step\n",
    "        g, count = batched_weighted_sum(\n",
    "            proc_returns_n2[:, 0] - proc_returns_n2[:, 1],\n",
    "            (noise.get(idx, policy.num_params) for idx in noise_inds_n),\n",
    "            batch_size=500\n",
    "        )\n",
    "        g /= returns_n2.size\n",
    "        assert g.shape == (policy.num_params,) and g.dtype == np.float32 and count == len(noise_inds_n)\n",
    "        update_ratio = optimizer.update(-g + config.l2coeff * theta)\n",
    "\n",
    "        # Update ob stat (we're never running the policy in the master, but we might be snapshotting the policy)\n",
    "        if policy.needs_ob_stat:\n",
    "            policy.set_ob_stat(ob_stat.mean, ob_stat.std)\n",
    "\n",
    "        # Update number of steps to take\n",
    "        if adaptive_tslimit and (lengths_n2 == tslimit).mean() >= incr_tslimit_threshold:\n",
    "            old_tslimit = tslimit\n",
    "            tslimit = int(tslimit_incr_ratio * tslimit)\n",
    "            logger.info('Increased timestep limit from {} to {}'.format(old_tslimit, tslimit))\n",
    "\n",
    "        step_tend = time.time()\n",
    "        tlogger.record_tabular(\"EpRewMean\", returns_n2.mean())\n",
    "        tlogger.record_tabular(\"EpRewStd\", returns_n2.std())\n",
    "        tlogger.record_tabular(\"EpLenMean\", lengths_n2.mean())\n",
    "\n",
    "        tlogger.record_tabular(\"EvalEpRewMean\", np.nan if not eval_rets else np.mean(eval_rets))\n",
    "        tlogger.record_tabular(\"EvalEpRewStd\", np.nan if not eval_rets else np.std(eval_rets))\n",
    "        tlogger.record_tabular(\"EvalEpLenMean\", np.nan if not eval_rets else np.mean(eval_lens))\n",
    "        tlogger.record_tabular(\"EvalPopRank\", np.nan if not eval_rets else (\n",
    "            np.searchsorted(np.sort(returns_n2.ravel()), eval_rets).mean() / returns_n2.size))\n",
    "        tlogger.record_tabular(\"EvalEpCount\", len(eval_rets))\n",
    "\n",
    "        tlogger.record_tabular(\"Norm\", float(np.square(policy.get_trainable_flat()).sum()))\n",
    "        tlogger.record_tabular(\"GradNorm\", float(np.square(g).sum()))\n",
    "        tlogger.record_tabular(\"UpdateRatio\", float(update_ratio))\n",
    "\n",
    "        tlogger.record_tabular(\"EpisodesThisIter\", lengths_n2.size)\n",
    "        tlogger.record_tabular(\"EpisodesSoFar\", episodes_so_far)\n",
    "        tlogger.record_tabular(\"TimestepsThisIter\", lengths_n2.sum())\n",
    "        tlogger.record_tabular(\"TimestepsSoFar\", timesteps_so_far)\n",
    "\n",
    "        num_unique_workers = len(set(worker_ids))\n",
    "        tlogger.record_tabular(\"UniqueWorkers\", num_unique_workers)\n",
    "        tlogger.record_tabular(\"UniqueWorkersFrac\", num_unique_workers / len(worker_ids))\n",
    "        tlogger.record_tabular(\"ResultsSkippedFrac\", frac_results_skipped)\n",
    "        tlogger.record_tabular(\"ObCount\", ob_count_this_batch)\n",
    "\n",
    "        tlogger.record_tabular(\"TimeElapsedThisIter\", step_tend - step_tstart)\n",
    "        tlogger.record_tabular(\"TimeElapsed\", step_tend - tstart)\n",
    "        tlogger.dump_tabular()\n",
    "\n",
    "        if config.snapshot_freq != 0 and curr_task_id % config.snapshot_freq == 0:\n",
    "            import os.path as osp\n",
    "            filename = osp.join(tlogger.get_dir(), 'snapshot_iter{:05d}_rew{}.h5'.format(\n",
    "                curr_task_id,\n",
    "                np.nan if not eval_rets else int(np.mean(eval_rets))\n",
    "            ))\n",
    "            assert not osp.exists(filename)\n",
    "            policy.save(filename)\n",
    "            tlogger.log('Saved snapshot {}'.format(filename))\n",
    "\n",
    "\n",
    "def rollout_and_update_ob_stat(policy, env, timestep_limit, rs, task_ob_stat, calc_obstat_prob):\n",
    "    if policy.needs_ob_stat and calc_obstat_prob != 0 and rs.rand() < calc_obstat_prob:\n",
    "        rollout_rews, rollout_len, obs = policy.rollout(\n",
    "            env, timestep_limit=timestep_limit, save_obs=True, random_stream=rs)\n",
    "        task_ob_stat.increment(obs.sum(axis=0), np.square(obs).sum(axis=0), len(obs))\n",
    "    else:\n",
    "        rollout_rews, rollout_len = policy.rollout(env, timestep_limit=timestep_limit, random_stream=rs)\n",
    "    return rollout_rews, rollout_len\n",
    "\n",
    "\n",
    "def run_worker(noise, exp, task_queue, result_queue, lock, *, min_task_runtime=.2):\n",
    "    \"\"\"\n",
    "    Starts a worker to work on the environment.\n",
    "\n",
    "    :param relay_redis_cfg: Path to the redis unix socket\n",
    "    :param noise: Object of SharedNoiseTable, all workers use the same to reconstruct the noise later by the master\n",
    "    :param min_task_runtime: Optional parameter to the state the minimum runtime per task\n",
    "    :return: None, results get pushed to the redis server\n",
    "    \"\"\"\n",
    "\n",
    "    with lock:\n",
    "        logger.info('run_worker: {}'.format(locals()))\n",
    "\n",
    "    assert isinstance(noise, SharedNoiseTable)\n",
    "\n",
    "    # Setup\n",
    "    config, env, sess, policy = setup(exp, single_threaded=True)\n",
    "\n",
    "    # Random stream used for todo\n",
    "    rs = np.random.RandomState()\n",
    "    worker_id = rs.randint(2 ** 31)\n",
    "\n",
    "    assert policy.needs_ob_stat == (config.calc_obstat_prob != 0)\n",
    "\n",
    "    while True:\n",
    "        task_data = get_task(task_queue, lock)\n",
    "\n",
    "        task_tstart = time.time()\n",
    "\n",
    "        assert isinstance(task_data, Task)\n",
    "        task_id = task_data.task_id\n",
    "        assert isinstance(task_id, int)\n",
    "\n",
    "        if policy.needs_ob_stat:\n",
    "            policy.set_ob_stat(task_data.ob_mean, task_data.ob_std)\n",
    "\n",
    "        # todo whats this condition doing?\n",
    "        if rs.rand() < config.eval_prob:\n",
    "            # Evaluation: noiseless weights and noiseless actions\n",
    "            policy.set_trainable_flat(task_data.params)\n",
    "\n",
    "            eval_rews, eval_length = policy.rollout(env)  # eval rollouts don't obey task_data.timestep_limit\n",
    "            eval_return = eval_rews.sum()\n",
    "\n",
    "            with lock:\n",
    "                logger.info('Eval result: task={} return={:.3f} length={}'.format(task_id, eval_return, eval_length))\n",
    "\n",
    "            result_queue.put(Result(\n",
    "                worker_id=worker_id,\n",
    "                noise_inds_n=None,\n",
    "                returns_n2=None,\n",
    "                signreturns_n2=None,\n",
    "                lengths_n2=None,\n",
    "                eval_return=eval_return,\n",
    "                eval_length=eval_length,\n",
    "                ob_sum=None,\n",
    "                ob_sumsq=None,\n",
    "                ob_count=None,\n",
    "                task_id=task_id\n",
    "            ))\n",
    "\n",
    "        else:\n",
    "            # Rollouts with noise\n",
    "            noise_inds, returns, signreturns, lengths = [], [], [], []\n",
    "            task_ob_stat = RunningStat(env.observation_space.shape, eps=0.)  # eps=0 because we're incrementing only\n",
    "\n",
    "            while not noise_inds or time.time() - task_tstart < min_task_runtime:\n",
    "                # Sample noise from the SharedNoise\n",
    "                noise_idx = noise.sample_index(rs, policy.num_params)\n",
    "                v = config.noise_stdev * noise.get(noise_idx, policy.num_params)\n",
    "\n",
    "                # Evaluate the sampled noise positive\n",
    "                policy.set_trainable_flat(task_data.params + v)\n",
    "                rews_pos, len_pos = rollout_and_update_ob_stat(\n",
    "                    policy, env, task_data.timestep_limit, rs, task_ob_stat, config.calc_obstat_prob)\n",
    "\n",
    "                # Evaluate the sample noise negative\n",
    "                policy.set_trainable_flat(task_data.params - v)\n",
    "                rews_neg, len_neg = rollout_and_update_ob_stat(\n",
    "                    policy, env, task_data.timestep_limit, rs, task_ob_stat, config.calc_obstat_prob)\n",
    "\n",
    "                # Gather results\n",
    "                noise_inds.append(noise_idx)\n",
    "                returns.append([rews_pos.sum(), rews_neg.sum()])\n",
    "                signreturns.append([np.sign(rews_pos).sum(), np.sign(rews_neg).sum()])\n",
    "                lengths.append([len_pos, len_neg])\n",
    "\n",
    "            result_queue.put(Result(\n",
    "                worker_id=worker_id,\n",
    "                noise_inds_n=np.array(noise_inds),\n",
    "                returns_n2=np.array(returns, dtype=np.float32),\n",
    "                signreturns_n2=np.array(signreturns, dtype=np.float32),\n",
    "                lengths_n2=np.array(lengths, dtype=np.int32),\n",
    "                eval_return=None,\n",
    "                eval_length=None,\n",
    "                ob_sum=None if task_ob_stat.count == 0 else task_ob_stat.sum,\n",
    "                ob_sumsq=None if task_ob_stat.count == 0 else task_ob_stat.sumsq,\n",
    "                ob_count=task_ob_stat.count,\n",
    "                task_id=task_id\n",
    "            ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import errno\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import click\n",
    "from multiprocessing import Process, Queue, Lock\n",
    "\n",
    "from es_distributed.es import run_master, run_worker, SharedNoiseTable\n",
    "\n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "@click.group()\n",
    "def cli():\n",
    "    logging.basicConfig(\n",
    "        format='[%(asctime)s pid=%(process)d] %(message)s',\n",
    "        level=logging.INFO,\n",
    "        stream=sys.stderr)\n",
    "\n",
    "\n",
    "@cli.command()\n",
    "@click.option('--exp_str') #configuration file\n",
    "@click.option('--exp_file')\n",
    "@click.option('--num_workers')\n",
    "@click.option('--log_dir')\n",
    "def master(exp_str, exp_file, num_workers, log_dir):\n",
    "    \"\"\"\n",
    "    Starts the master which will listen for workers on the redis server.\n",
    "\n",
    "    \"master\" must be used as first parameter when starting main.py, then this method gets called. In addition,\n",
    "    exp_str xor exp_file must be provided, which is the configuration file and master_socket_path, the socket\n",
    "    for the redis server is also mandatory.\n",
    "\n",
    "    :param exp_str: A JSON formatted string containing the configuration\n",
    "    :param exp_file: A JSON file containing the configuration\n",
    "    :param master_socket_path: Path to the unixsocket used by redis\n",
    "    :param log_dir: Optional to specify where to save logs\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Start the master\n",
    "    # JSON String XOR JSON File\n",
    "    assert (exp_str is None) != (exp_file is None), 'Must provide exp_str xor exp_file to the master'\n",
    "    if exp_str:\n",
    "        exp = json.loads(exp_str)\n",
    "    elif exp_file:\n",
    "        with open(exp_file, 'r') as f:\n",
    "            exp = json.loads(f.read())\n",
    "    else:\n",
    "        assert False\n",
    "    log_dir = os.path.expanduser(log_dir) if log_dir else '/tmp/es_master_{}'.format(os.getpid())\n",
    "    mkdir_p(log_dir)\n",
    "\n",
    "    noise = SharedNoiseTable()  # Workers share the same noise so less data needs to be interchanged\n",
    "    num_workers = num_workers if num_workers else os.cpu_count()\n",
    "\n",
    "    task_queue = Queue()\n",
    "    result_queue = Queue()\n",
    "\n",
    "    lock = Lock()\n",
    "\n",
    "    workers = []\n",
    "\n",
    "    for _ in range(int(num_workers)):\n",
    "        worker_p = Process(target=run_worker, args=(noise, exp, task_queue, result_queue, lock,))\n",
    "        workers.append(worker_p)\n",
    "        worker_p.start()\n",
    "\n",
    "    master_p = Process(target=run_master, args=(exp, task_queue, result_queue, lock, log_dir,))\n",
    "    master_p.start()\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "    \n",
    "    master_p.join()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cli()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
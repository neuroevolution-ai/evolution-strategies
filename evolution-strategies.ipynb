{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIT License\n",
    "\n",
    "Copyright (c) 2016 OpenAI (http://openai.com)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in\n",
    "all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using evolution strategies to train Roboschool Environments\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook is based on the [paper](https://arxiv.org/abs/1703.03864), [blog article](https://openai.com/blog/evolution-strategies/) and [implementation](https://github.com/openai/evolution-strategies-starter) of OpenAI on the topic of using an evolution strategy algorithm for a typical reinforcement learning task. \n",
    "\n",
    "My implementation summarizes their implementation, by simplifying, refactoring and organizing the code into this Jupyter notebook which can be used to test the algorithm. One can tweak the hyperparameters, change the environment which shall be trained or even expand the implementation to support for example Atari environments.\n",
    "\n",
    "I recommend reading the paper or at least the article before trying out the notebook. Also depending on the environment the training can be very computationally intense (for example training the Humanoid), so if you want to try out the harder ones I recommend using a highly parallelizable machine, i.e. a machine with a high number of cores/threads.\n",
    "\n",
    "## Algorithm overview\n",
    "\n",
    "This section gives a brief overview over the used strategy. First of all we need to define what this implementation is going to do. The Roboschool is a group of environments in the [OpenAi Gym](https://gym.openai.com/), a program to test the behavior of machine learning algorithms on _real world_ problems. In our case, we want to train different robotic environments using evolution strategies. We therefore define a neural net with a configurable number of hidden layers, where the input dimension equals the observation space of the environment and the dimension of the output layer equals the dimension of the action space of the environment. This neural net will be called policy in our context. Therefore we train our policy to output the best possible action sequence given an observation sequence. Now, how do we train this policy? Training an evolutionary strategy consists of a cycle which is repeated over and over. First, an initial weight vector is randomly generated. Then we perturb this vector. The number of perturbations is called the population size. Lets say we have a population size of 100. Therefore we now have 100 weight vectors which are slightly different from the initial one. We then take these new weight vectors and evaluate them on our environment. The environment gives us a reward back, a number which indicates how well the policy with our perturbed weight vectors has done. So now we have 100 perturbed weight vectors with 100 noise vectors and 100 rewards for each perturbed weight vector. We then sum over all the rewards and weight them with the used noise. We then multiply this sum with the learning rate and divide by the population size and the noise standard deviation. The result of this equation gives us the new step which we add to the inital weight vector. Then we have the initial weight vector for the next cycle. We call such a cycle a generation. What it essentially does is evaluate which noise we added resulted in the best reward and we want the next generation to base on that noise.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before starting any computation we need to configure the program and define some methods and objects we will use later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configuration & Result Classes\n",
    "\n",
    "Using a `namedtuple` allows use to quickly create a class with different attributes, which is ideal for defining a Config and Result Class.\n",
    "\n",
    "The Config class defines our general configuration of the program. The following table explains what each attribute does:\n",
    "\n",
    "\n",
    "| Attribute             | Explanation   |\n",
    "| :---------------------|:--------------|\n",
    "| `env_id`              | A string representing an environment of the Roboschool, e.g. `\"RoboschoolAnt-v1\"`|\n",
    "| `population_size`     | The size of the population of one generation    |\n",
    "| `num_workers`         | The number of workers doing computation in parallel, for maximum performance the default is `os.cpu_cores`|\n",
    "| `learning_rate`       | A floating point number which dictates how _strong_ the best candidate of a generation shall be influencing the paramter vector \n",
    "| `noise_stdev`         |\n",
    "| `snapshot_freq`       | An integer which tells in which frequency the weights of the model shall be saved, e.g. a snapshot frequency of 10 would save the weights of the model every 10 generations\n",
    "| `return_proc_mode`    | The processing mode of the gathered returns, currentyl only `\"centered_rank\"` is available\n",
    "| `timesteps_per_batch` | \n",
    "| `calc_obstat_prob`    | The probability of calculating the new mean and standard deviation of the observation space\n",
    "| `eval_prob`           | The probability of inserting an evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Config = namedtuple('Config', [\n",
    "    'env_id',\n",
    "    'population_size',\n",
    "    'num_workers',\n",
    "    'learning_rate',\n",
    "    'noise_stdev',\n",
    "    'snapshot_freq',\n",
    "    'return_proc_mode',\n",
    "    #'timesteps_per_batch',\n",
    "    'calc_obstat_prob'\n",
    "    #'eval_prob'\n",
    "])\n",
    "\n",
    "Result = namedtuple('Result', [\n",
    "    'worker_id',\n",
    "    'noise_inds_n','returns_n2', 'signreturns_n2', 'lengths_n2',\n",
    "    'eval_return', 'eval_length',\n",
    "    'ob_sum', 'ob_sumsq', 'ob_count',\n",
    "    'task_id'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config = Config(\n",
    "    env_id=\"RoboschoolAnt-v1\",\n",
    "    population_size=32,\n",
    "    num_workers=os.cpu_count(),\n",
    "    learning_rate=0.005,\n",
    "    noise_stdev=0.02,\n",
    "    snapshot_freq=1,\n",
    "    return_proc_mode=\"centered_rank\",\n",
    "    calc_obstat_prob=0.01\n",
    ")\n",
    "\n",
    "#config.num_workers = config.num_workers if config.num_workers else os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment\n",
    "\n",
    "Create one for every worker -> done in worker method\n",
    "Master also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym, roboschool # Roboschool import needed to register the environments within gym\n",
    "env = gym.make(config.env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorflow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Policy setup\n",
    "\n",
    "Currently saves the arguments as local variable, then creates a TensorFlow variable scope where the neural network\n",
    "architecture gets created.\n",
    "\n",
    "Currently emitted:\n",
    "1. Observation normalization\n",
    "2. Obseration clipping\n",
    "3. _act function\n",
    "6. set_all_vars\n",
    "\n",
    "## Keras as Model\n",
    "\n",
    "Original implementation used hand written dense layers and tensorflow operations. I use a Keras model and their\n",
    "functional API to create the net. In testing the two version differ in 0.x float scope. Something to worry about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "args = {\n",
    "      \"ac_bins\": \"continuous:\",\n",
    "      \"ac_noise_std\": 0.01,\n",
    "      #\"connection_type\": \"ff\",\n",
    "      \"hidden_dims\": [\n",
    "        256,\n",
    "        256\n",
    "      ],\n",
    "      \"nonlin_type\": \"tanh\"\n",
    "}\n",
    "\n",
    "ob_space= env.observation_space\n",
    "ac_space = env.action_space\n",
    "ac_bins = args[\"ac_bins\"]\n",
    "ac_noise_std = args[\"ac_noise_std\"]\n",
    "hidden_dims = args[\"hidden_dims\"]\n",
    "nonlin = args[\"nonlin_type\"]\n",
    "\n",
    "# TODO more nonlinear functions\n",
    "\n",
    "class RunningStat(object):\n",
    "    def __init__(self, shape, eps):\n",
    "        self.sum = np.zeros(shape, dtype=np.float32)\n",
    "        self.sumsq = np.full(shape, eps, dtype=np.float32)\n",
    "        self.count = eps\n",
    "\n",
    "    def increment(self, s, ssq, c):\n",
    "        self.sum += s\n",
    "        self.sumsq += ssq\n",
    "        self.count += c\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.sum / self.count\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(np.maximum(self.sumsq / self.count - np.square(self.mean), 1e-2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keras clearin backend to support multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(initial_weights=None, model_name=\"model\", save_path=None):\n",
    "    #tf.keras.backend.clear_session()\n",
    "    import tensorflow as tf\n",
    "    nonlin = tf.tanh\n",
    "\n",
    "    with tf.variable_scope(\"RoboschoolPolicy/\" + model_name):\n",
    "        # Observation normalization\n",
    "        #ob_mean = tf.get_variable(\n",
    "        #    'ob_mean', ob_space.shape, tf.float32, tf.constant_initializer(np.nan), trainable=False)\n",
    "        #ob_std = tf.get_variable(\n",
    "        #    'ob_std', ob_space.shape, tf.float32, tf.constant_initializer(np.nan), trainable=False)\n",
    "        #in_mean = tf.placeholder(tf.float32, ob_space.shape)\n",
    "        #in_std = tf.placeholder(tf.float32, ob_space.shape)\n",
    "        #self._set_ob_mean_std = U.function([in_mean, in_std], [], updates=[\n",
    "            #tf.assign(ob_mean, in_mean),\n",
    "            #tf.assign(ob_std, in_std),\n",
    "        #])\n",
    "\n",
    "        # Normalize observation space and clip to [-5.0, 5.0]\n",
    "        #o = tf.clip_by_value((o - ob_mean) / ob_std, -5.0, 5.0)\n",
    "\n",
    "        # Policy network\n",
    "\n",
    "        #input = x = tf.keras.Input(ob_space.shape, dtype=tf.float32)\n",
    "        input = x = tf.keras.Input(ob_space.shape, dtype=tf.float32)\n",
    "\n",
    "        for hd in hidden_dims:\n",
    "            x = tf.keras.layers.Dense(\n",
    "                hd, activation=nonlin,\n",
    "                kernel_initializer=tf.initializers.random_normal,\n",
    "                bias_initializer=tf.initializers.zeros)(x)\n",
    "\n",
    "        # Map to action\n",
    "        adim = ac_space.shape[0]\n",
    "\n",
    "        a = tf.keras.layers.Dense(\n",
    "        adim,\n",
    "        kernel_initializer=tf.initializers.random_normal,\n",
    "        bias_initializer=tf.initializers.zeros)(x)\n",
    "        model = tf.keras.Model(inputs=input, outputs=a, name=model_name)\n",
    "\n",
    "        # Initializer for the newly created weights. TODO possible replacement tf.keras.initializers.RandomNormal\n",
    "        # out = np.random.randn(*adim).astype(np.float32)\n",
    "        # out *=  0.01 / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        # initializer= tf.constant(out)\n",
    "\n",
    "\n",
    "    if initial_weights is not None:\n",
    "        set_from_flat(model, initial_weights)\n",
    "        \n",
    "    if save_path:\n",
    "        model.save_weights(save_path)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               7424      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 2056      \n",
      "=================================================================\n",
      "Total params: 75,272\n",
      "Trainable params: 75,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def act(ob, model, random_stream=None, ob_mean=None, ob_std=None):\n",
    "    ob_proc = ob\n",
    "    if ob_mean is not None and ob_std is not None:\n",
    "        ob_proc = np.clip((ob - ob_mean) / ob_std, -5.0, 5.0)\n",
    "    action = model.predict(ob)\n",
    "    \n",
    "    # TODO why randomstream? Better generalization?\n",
    "    if random_stream is not None and ac_noise_std != 0:\n",
    "        action += random_stream.randn(*action.shape) * ac_noise_std\n",
    "    return action\n",
    "\n",
    "def get_initial_weights():\n",
    "    model = create_model()\n",
    "    \n",
    "    # Print out the model\n",
    "    model.summary()\n",
    "    \n",
    "    return model.get_weights()\n",
    "\n",
    "# Plot the Neural Network Architecture\n",
    "#master_model.summary()\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool(1) as pool:\n",
    "    theta = pool.apply(func=get_initial_weights)\n",
    "\n",
    "\n",
    "# Plot the Neural Network Architecture\n",
    "#master_model.summary()\n",
    "#all_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope.name)\n",
    "\n",
    "\n",
    "#trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, master_scope.name)\n",
    "#trainable_variables = model.get_weights()\n",
    "\n",
    "num_params = sum(np.prod(v.shape) for v in theta)\n",
    "\n",
    "#placeholders = [tf.placeholder(v.value().dtype, v.get_shape().as_list()) for v in self.all_variables]\n",
    "\n",
    "# self.set_all_vars = U.function(\n",
    "#     inputs=placeholders,\n",
    "#     outputs=[],\n",
    "#     updates=[tf.group(*[v.assign(p) for v, p in zip(self.all_variables, placeholders)])]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#optimizer = {'sgd': SGD, 'adam': Adam}[exp['optimizer']['type']](policy, **exp['optimizer']['args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Shared Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SharedNoiseTable(object):\n",
    "    def __init__(self):\n",
    "        import ctypes, multiprocessing\n",
    "        seed = 123\n",
    "        count = 250000000  # 1 gigabyte of 32-bit numbers. Will actually sample 2 gigabytes below.\n",
    "        #logger.info('Sampling {} random numbers with seed {}'.format(count, seed))\n",
    "\n",
    "        # Instantiate an array of C float datatype with size count\n",
    "        self._shared_mem = multiprocessing.Array(ctypes.c_float, count)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        self.noise = np.ctypeslib.as_array(self._shared_mem.get_obj())\n",
    "        assert self.noise.dtype == np.float32\n",
    "        self.noise[:] = np.random.RandomState(seed).randn(count)  # 64-bit to 32-bit conversion here\n",
    "        #logger.info('Sampled {} bytes'.format(self.noise.size * 4))\n",
    "\n",
    "    def get(self, i, dim):\n",
    "        return self.noise[i:i + dim]\n",
    "\n",
    "    def sample_index(self, stream, dim):\n",
    "        return stream.randint(0, len(self.noise) - dim + 1)\n",
    "\n",
    "noise = SharedNoiseTable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def set_new_weights(model, theta, epsilon):\n",
    "#     assert isinstance(model, tf.keras.Model)\n",
    "#     assert isinstance(theta, list)\n",
    "#         \n",
    "#     for t in theta:\n",
    "#         t += epsilon\n",
    "#     \n",
    "#     model.set_weights(theta)\n",
    "\n",
    "def get_flat(theta):\n",
    "     return np.concatenate([np.reshape(v, [-1]) for v in theta], 0)\n",
    "\n",
    "def set_from_flat(model, theta):\n",
    "    old_theta = model.get_weights()\n",
    "    shapes = [v.shape for v in old_theta]\n",
    "    total_size = theta.size\n",
    "    \n",
    "    start = 0\n",
    "    reshapes = []\n",
    "    \n",
    "    for (shape, v) in zip(shapes, theta):\n",
    "        size = int(np.prod(shape))\n",
    "        reshapes.append(np.reshape(theta[start:start+size], shape))\n",
    "        start += size\n",
    "    \n",
    "    assert start == total_size\n",
    "    model.set_weights(reshapes)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set from flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def _create_set_from_flat_op(var_list, orig):\n",
    "#     shapes = [v.shape for v in orig]\n",
    "#     total_size = np.sum([v.size for v in orig])\n",
    "#         \n",
    "#     start=0\n",
    "#     assigns = []\n",
    "#     for (shape, v) in zip(shapes, var_list):\n",
    "#         size = v.size\n",
    "#         assigns.append(np.reshape(var_list[start:start+size], shape))\n",
    "#         start += size\n",
    "#         \n",
    "#     assert start == total_size\n",
    "#     \n",
    "#     return assigns\n",
    "#  \n",
    "# \n",
    "# def set_from_flat(var_list):\n",
    "#     old_weights = get_flat(model.get_weights())\n",
    "# \n",
    "#     new_weights = old_weights + var_list\n",
    "#     op_set_from_flat = _create_set_from_flat_op(new_weights)\n",
    "#     model.set_weights(sess.run(op_set_from_flat))\n",
    "#     \n",
    "#     print(\"PID \" + str(os.getpid()) + \": \" + \"Set weights from flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Rollout TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rollout(env, \n",
    "            model, \n",
    "            *, \n",
    "            render=False, \n",
    "            timestep_limit=None, \n",
    "            save_obs=False, \n",
    "            random_stream=None, \n",
    "            ob_mean=None, \n",
    "            ob_std=None):\n",
    "    \"\"\"\n",
    "    If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "    Otherwise, no action noise will be added.\n",
    "    \"\"\"\n",
    "    \n",
    "    env_timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "    rews = []\n",
    "    t = 0\n",
    "    if save_obs:\n",
    "        obs = []\n",
    "    ob = env.reset()\n",
    "    for _ in range(timestep_limit):\n",
    "        ac = act(ob[None], model, random_stream=random_stream, ob_mean=ob_mean, ob_std=ob_std)[0]\n",
    "        if save_obs:\n",
    "            obs.append(ob)\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "        rews.append(rew)\n",
    "        t += 1\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "    rews = np.array(rews, dtype=np.float32)\n",
    "    if save_obs:\n",
    "        return rews, t, np.array(obs)\n",
    "    return rews, t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Worker method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_and_update_ob_stat(env, model, rs, task_ob_stat, ob_mean=None, ob_std=None):\n",
    "    if ob_mean is not None and ob_std is not None and config.calc_obstat_prob != 0 and rs.rand() < config.calc_obstat_prob:\n",
    "        rollout_rews, rollout_len, obs = rollout(\n",
    "            env, model, save_obs=True, random_stream=rs)\n",
    "        task_ob_stat.increment(obs.sum(axis=0), np.square(obs).sum(axis=0), len(obs))\n",
    "    else:\n",
    "        rollout_rews, rollout_len = rollout(env, model, random_stream=rs)\n",
    "    return rollout_rews, rollout_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_worker(num_jobs, theta, ob_mean=None, ob_std=None): #min_task_runtime=.2):\n",
    "\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Started worker with \" + str(num_jobs) + \"Jobs\")\n",
    "    #with lock:\n",
    "    #    logger.info('run_worker: {}'.format(locals()))\n",
    "\n",
    "    assert isinstance(noise, SharedNoiseTable)\n",
    "\n",
    "    # Setup\n",
    "    #config, env, sess, policy = setup(exp, single_threaded=True)\n",
    "    env = gym.make(config.env_id)\n",
    "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
    "    \n",
    "\n",
    "    # Random stream used for todo\n",
    "    rs = np.random.RandomState()\n",
    "    #worker_id = rs.randint(2 ** 31)\n",
    "\n",
    "    #assert policy.needs_ob_stat == (config.calc_obstat_prob != 0)\n",
    "\n",
    "    #while True:\n",
    "    # Prevent accessing empty array (master did not emit task yet)\n",
    "    #while not tasks:\n",
    "    #    time.sleep(0.05)\n",
    "\n",
    "    #task_data = tasks[-1]\n",
    "\n",
    "    #task_tstart = time.time()\n",
    "\n",
    "    #assert isinstance(task_data, Task)\n",
    "    #task_id = task_data.task_id\n",
    "    #assert isinstance(task_id, int)\n",
    "\n",
    "    #if policy.needs_ob_stat:\n",
    "    #    policy.set_ob_stat(task_data.ob_mean, task_data.ob_std)\n",
    "\n",
    "    # # todo whats this condition doing?\n",
    "    # if rs.rand() < config.eval_prob:\n",
    "    #     # Evaluation: noiseless weights and noiseless actions\n",
    "    #     policy.set_trainable_flat(task_data.params)\n",
    "    # \n",
    "    #     eval_rews, eval_length = policy.rollout(env)  # eval rollouts don't obey task_data.timestep_limit\n",
    "    #     eval_return = eval_rews.sum()\n",
    "    # \n",
    "    #     with lock:\n",
    "    #         logger.info('Eval result: task={} return={:.3f} length={}'.format(task_id, eval_return, eval_length))\n",
    "    # \n",
    "    #     result_queue.put(Result(\n",
    "    #         worker_id=worker_id,\n",
    "    #         noise_inds_n=None,\n",
    "    #         returns_n2=None,\n",
    "    #         signreturns_n2=None,\n",
    "    #         lengths_n2=None,\n",
    "    #         eval_return=eval_return,\n",
    "    #         eval_length=eval_length,\n",
    "    #         ob_sum=None,\n",
    "    #         ob_sumsq=None,\n",
    "    #         ob_count=None,\n",
    "    #         task_id=task_id\n",
    "    #     ))\n",
    "\n",
    "    # Rollouts with noise\n",
    "    noise_inds, returns, signreturns, lengths = [], [], [], []\n",
    "    task_ob_stat = RunningStat(env.observation_space.shape, eps=0.)  # eps=0 because we're incrementing only\n",
    "    \n",
    "    #while not noise_inds or time.time() - task_tstart < min_task_runtime:\n",
    "    \n",
    "    for _ in range(num_jobs):\n",
    "\n",
    "        # ------------- Noise sample -------------------------------\n",
    "        noise_idx = noise.sample_index(rs, num_params)\n",
    "        epsilon = config.noise_stdev * noise.get(noise_idx, num_params)\n",
    "\n",
    "        # Evaluate the sampled noise positive\n",
    "        set_from_flat(model, theta + epsilon)\n",
    "        rews_pos, len_pos = rollout_and_update_ob_stat(env, model, rs=rs, task_ob_stat=task_ob_stat, ob_mean=ob_mean, ob_std=ob_std)\n",
    "\n",
    "        # rews_pos, len_pos = rollout_and_update_ob_stat(\n",
    "        #     policy, env, task_data.timestep_limit, rs, task_ob_stat, config.calc_obstat_prob)\n",
    "        \n",
    "        # Evaluate the sample noise negative\n",
    "        set_from_flat(model, theta - epsilon)\n",
    "        rews_neg, len_neg = rollout_and_update_ob_stat(env, model, rs=rs, task_ob_stat=task_ob_stat, ob_mean=ob_mean, ob_std=ob_std)\n",
    "\n",
    "        # rews_neg, len_neg = rollout_and_update_ob_stat(\n",
    "        #     policy, env, task_data.timestep_limit, rs, task_ob_stat, config.calc_obstat_prob)\n",
    "        \n",
    "    \n",
    "        # Gather results\n",
    "        noise_inds.append(noise_idx)\n",
    "        returns.append([rews_pos.sum(), rews_neg.sum()])\n",
    "        signreturns.append([np.sign(rews_pos).sum(), np.sign(rews_neg).sum()])\n",
    "        lengths.append([len_pos, len_neg])\n",
    "        \n",
    "        \n",
    "    # result_queue.put(Result(\n",
    "    #     worker_id=worker_id,\n",
    "    #     noise_inds_n=np.array(noise_inds),\n",
    "    #     returns_n2=np.array(returns, dtype=np.float32),\n",
    "    #     signreturns_n2=np.array(signreturns, dtype=np.float32),\n",
    "    #     lengths_n2=np.array(lengths, dtype=np.int32),\n",
    "    #     eval_return=None,\n",
    "    #     eval_length=None,\n",
    "    #     ob_sum=None if task_ob_stat.count == 0 else task_ob_stat.sum,\n",
    "    #     ob_sumsq=None if task_ob_stat.count == 0 else task_ob_stat.sumsq,\n",
    "    #     ob_count=task_ob_stat.count,\n",
    "    #     task_id=task_id\n",
    "    # ))\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Returned result\")\n",
    "    result = Result(\n",
    "        worker_id=None,\n",
    "        noise_inds_n=np.array(noise_inds),\n",
    "        returns_n2=np.array(returns, dtype=np.float32),\n",
    "        signreturns_n2=np.array(signreturns, dtype=np.float32),\n",
    "        lengths_n2=np.array(lengths, dtype=np.int32),\n",
    "        eval_return=None,\n",
    "        eval_length=None,\n",
    "        ob_sum=None if task_ob_stat.count == 0 else task_ob_stat.sum,\n",
    "        ob_sumsq=None if task_ob_stat.count == 0 else task_ob_stat.sumsq,\n",
    "        ob_count=task_ob_stat.count,\n",
    "        task_id = 0\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def itergroups(items, group_size):\n",
    "    assert group_size >= 1\n",
    "    group = []\n",
    "    for x in items:\n",
    "        group.append(x)\n",
    "        if len(group) == group_size:\n",
    "            yield tuple(group)\n",
    "            del group[:]\n",
    "    if group:\n",
    "        yield tuple(group)\n",
    "        \n",
    "def batched_weighted_sum(weights, vecs, batch_size):\n",
    "    total = 0.\n",
    "    num_items_summed = 0\n",
    "    for batch_weights, batch_vecs in zip(itergroups(weights, batch_size), itergroups(vecs, batch_size)):\n",
    "        assert len(batch_weights) == len(batch_vecs) <= batch_size\n",
    "        total += np.dot(np.asarray(batch_weights, dtype=np.float32), np.asarray(batch_vecs, dtype=np.float32))\n",
    "        num_items_summed += len(batch_weights)\n",
    "    return total, num_items_summed\n",
    "\n",
    "import errno\n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: Fitness shaping with a rank transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_ranks(x):\n",
    "    \"\"\"\n",
    "    Returns ranks in [0, len(x))\n",
    "    Note: This is different from scipy.stats.rankdata, which returns ranks in [1, len(x)].\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "    ranks = np.empty(len(x), dtype=int)\n",
    "    ranks[x.argsort()] = np.arange(len(x))\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def compute_centered_ranks(x):\n",
    "    y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: Using Adam Optimizer\n",
    "\n",
    "Defining it manually since with Keras you have to define a loss function and use training set, etc. Manually seems\n",
    "easier for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self):\n",
    "        self.dim = num_params\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, globalg):\n",
    "        self.t += 1\n",
    "        step = self._compute_step(globalg)\n",
    "        ratio = np.linalg.norm(step) / np.linalg.norm(theta)\n",
    "        theta_new = theta + step\n",
    "        return theta_new, ratio\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, stepsize, beta1=0.9, beta2=0.999, epsilon=1e-08):\n",
    "        Optimizer.__init__(self)\n",
    "        self.stepsize = stepsize\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = np.zeros(self.dim, dtype=np.float32)\n",
    "        self.v = np.zeros(self.dim, dtype=np.float32)\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        a = self.stepsize * np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * globalg\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (globalg * globalg)\n",
    "        step = -a * self.m / (np.sqrt(self.v) + self.epsilon)\n",
    "        return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------GENERATION: 0------------------------------------\n",
      "PID 28168: Started worker with 4Jobs\n",
      "PID 28169: Started worker with 4Jobs\n",
      "PID 28171: Started worker with 4Jobs\n",
      "PID 28170: Started worker with 4Jobs\n",
      "PID 28172: Started worker with 4Jobs\n",
      "PID 28173: Started worker with 4Jobs\n",
      "PID 28175: Started worker with 4Jobs\n",
      "PID 28174: Started worker with 4Jobs\n",
      "PID 10896: Waiting for results\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "PID 28169: Returned result\n",
      "PID 28173: Returned result\n",
      "PID 28174: Returned result\n",
      "PID 28172: Returned result\n",
      "PID 28168: Returned result\n",
      "PID 28170: Returned result\n",
      "PID 28175: Returned result\n",
      "PID 28171: Returned result\n",
      "Gathered results\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Saved model in generation 0\n",
      "----------------------GENERATION: 1------------------------------------\n",
      "PID 28748: Started worker with 4Jobs\n",
      "PID 28749: Started worker with 4Jobs\n",
      "PID 28750: Started worker with 4Jobs\n",
      "PID 28751: Started worker with 4Jobs\n",
      "PID 28752: Started worker with 4Jobs\n",
      "PID 28753: Started worker with 4Jobs\n",
      "PID 28754: Started worker with 4Jobs\n",
      "PID 28755: Started worker with 4Jobs\n",
      "PID 10896: Waiting for results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-22c226841d65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "  File \"<ipython-input-16-e904f213e502>\", line 12, in run_worker\n",
      "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
      "  File \"<ipython-input-16-e904f213e502>\", line 12, in run_worker\n",
      "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/pdeubel/PycharmProjects/evolution-strategies/matplotlibrc'\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-8-fa431a5d7726>\", line 3, in create_model\n",
      "    import tensorflow as tf\n",
      "  File \"<ipython-input-16-e904f213e502>\", line 12, in run_worker\n",
      "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
      "  File \"<ipython-input-16-e904f213e502>\", line 12, in run_worker\n",
      "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
      "  File \"<ipython-input-8-fa431a5d7726>\", line 3, in create_model\n",
      "    import tensorflow as tf\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  File \"<ipython-input-16-e904f213e502>\", line 12, in run_worker\n",
      "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
      "  File \"<ipython-input-8-fa431a5d7726>\", line 3, in create_model\n",
      "    import tensorflow as tf\n",
      "  File \"<ipython-input-8-fa431a5d7726>\", line 3, in create_model\n",
      "    import tensorflow as tf\n",
      "  File \"<ipython-input-8-fa431a5d7726>\", line 3, in create_model\n",
      "    import tensorflow as tf\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 29, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 29, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 29, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 29, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "Process ForkPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 29, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 626, in <module>\n",
      "    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py\", line 56, in package_hook\n",
      "    child_pkg = importlib.import_module(child_package_str)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 626, in <module>\n",
      "    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 626, in <module>\n",
      "    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 626, in <module>\n",
      "    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py\", line 56, in package_hook\n",
      "    child_pkg = importlib.import_module(child_package_str)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-16-e904f213e502>\", line 12, in run_worker\n",
      "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py\", line 56, in package_hook\n",
      "    child_pkg = importlib.import_module(child_package_str)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py\", line 56, in package_hook\n",
      "    child_pkg = importlib.import_module(child_package_str)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1 import estimator\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1 import estimator\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 626, in <module>\n",
      "    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))\n",
      "  File \"<ipython-input-8-fa431a5d7726>\", line 3, in create_model\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1.estimator import experimental\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py\", line 56, in package_hook\n",
      "    child_pkg = importlib.import_module(child_package_str)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 29, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"<ipython-input-16-e904f213e502>\", line 12, in run_worker\n",
      "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 626, in <module>\n",
      "    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1 import estimator\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1.estimator import experimental\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-8-fa431a5d7726>\", line 3, in create_model\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py\", line 56, in package_hook\n",
      "    child_pkg = importlib.import_module(child_package_str)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1.estimator import experimental\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1 import estimator\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1 import estimator\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1 import estimator\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\n",
      "    import tensorflow_estimator.python.estimator.estimator_lib\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1.estimator import experimental\n",
      "  File \"<ipython-input-16-e904f213e502>\", line 12, in run_worker\n",
      "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1.estimator import experimental\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1.estimator import experimental\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\n",
      "    import tensorflow_estimator.python.estimator.estimator_lib\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 53, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs import inputs\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\n",
      "    import tensorflow_estimator.python.estimator.estimator_lib\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 53, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs import inputs\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/inputs.py\", line 22, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.numpy_io import numpy_input_fn\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\n",
      "    import tensorflow_estimator.python.estimator.estimator_lib\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/inputs.py\", line 22, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.numpy_io import numpy_input_fn\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 53, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs import inputs\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.queues import feeding_functions\n",
      "  File \"<ipython-input-8-fa431a5d7726>\", line 3, in create_model\n",
      "    import tensorflow as tf\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\n",
      "    import tensorflow_estimator.python.estimator.estimator_lib\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 53, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs import inputs\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 29, in <module>\n",
      "    from tensorflow._api.v1 import compat\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/inputs.py\", line 22, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.numpy_io import numpy_input_fn\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.queues import feeding_functions\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.queues import feeding_functions\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/__init__.py\", line 55, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/api.py\", line 24, in <module>\n",
      "    from pandas.core.groupby import Grouper, NamedAgg\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.groupby.generic import (  # noqa: F401\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 53, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs import inputs\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/inputs.py\", line 22, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.numpy_io import numpy_input_fn\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/generic.py\", line 52, in <module>\n",
      "    from pandas.core.sparse.frame import SparseDataFrame\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/inputs.py\", line 22, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.numpy_io import numpy_input_fn\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/__init__.py\", line 29, in <module>\n",
      "    from tensorflow._api.v1 import compat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/__init__.py\", line 147, in <module>\n",
      "    from pandas.io.api import (\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 626, in <module>\n",
      "    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.queues import feeding_functions\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\n",
      "    import tensorflow_estimator.python.estimator.estimator_lib\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py\", line 56, in package_hook\n",
      "    child_pkg = importlib.import_module(child_package_str)\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/sparse/frame.py\", line 1088, in <module>\n",
      "    ops.add_flex_arithmetic_methods(SparseDataFrame)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 53, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs import inputs\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/ops/__init__.py\", line 873, in add_flex_arithmetic_methods\n",
      "    cls, flex_arith_method, flex_comp_method, bool_method=None, special=False\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.queues import feeding_functions\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/__init__.py\", line 21, in <module>\n",
      "    from tensorflow._api.v1.compat import v1\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/ops/__init__.py\", line 760, in _create_methods\n",
      "    ne=comp_method(cls, operator.ne, special),\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/__init__.py\", line 55, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/ops/__init__.py\", line 1531, in _flex_comp_method_FRAME\n",
      "    def f(self, other, axis=default_axis, level=None):\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 327, in __call__\n",
      "    func.__doc__ = dedent(self.join.join(docitems))\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/io/api.py\", line 17, in <module>\n",
      "    from pandas.io.pytables import HDFStore, read_hdf\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/api.py\", line 24, in <module>\n",
      "    from pandas.core.groupby import Grouper, NamedAgg\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/__init__.py\", line 55, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/usr/lib/python3.7/textwrap.py\", line 430, in dedent\n",
      "    text = _whitespace_only_re.sub('', text)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/_api/v1/compat/v1/__init__.py\", line 626, in <module>\n",
      "    child_package_str=('tensorflow_estimator.python.estimator.api.estimator'))\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.groupby.generic import (  # noqa: F401\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/api.py\", line 24, in <module>\n",
      "    from pandas.core.groupby import Grouper, NamedAgg\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow/python/tools/component_api_helper.py\", line 56, in package_hook\n",
      "    child_pkg = importlib.import_module(child_package_str)\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/generic.py\", line 44, in <module>\n",
      "    from pandas.core.frame import DataFrame\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/frame.py\", line 114, in <module>\n",
      "    from pandas.core.series import Series\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/inputs.py\", line 22, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.numpy_io import numpy_input_fn\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1 import estimator\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/series.py\", line 141, in <module>\n",
      "    class Series(base.IndexOpsMixin, generic.NDFrame):\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/series.py\", line 4215, in Series\n",
      "    def reindex(self, index=None, **kwargs):\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1.estimator import experimental\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.queues import feeding_functions\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/__init__.py\", line 55, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/util/_decorators.py\", line 327, in __call__\n",
      "    func.__doc__ = dedent(self.join.join(docitems))\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.groupby.generic import (  # noqa: F401\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/__init__.py\", line 55, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/generic.py\", line 44, in <module>\n",
      "    from pandas.core.frame import DataFrame\n",
      "  File \"/usr/lib/python3.7/textwrap.py\", line 431, in dedent\n",
      "    indents = _leading_whitespace_re.findall(text)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/api.py\", line 24, in <module>\n",
      "    from pandas.core.groupby import Grouper, NamedAgg\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\n",
      "    import tensorflow_estimator.python.estimator.estimator_lib\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 53, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs import inputs\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/frame.py\", line 114, in <module>\n",
      "    from pandas.core.series import Series\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.groupby.generic import (  # noqa: F401\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/api.py\", line 24, in <module>\n",
      "    from pandas.core.groupby import Grouper, NamedAgg\n",
      "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/inputs.py\", line 22, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.numpy_io import numpy_input_fn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.groupby.generic import (  # noqa: F401\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/generic.py\", line 44, in <module>\n",
      "    from pandas.core.frame import DataFrame\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/frame.py\", line 114, in <module>\n",
      "    from pandas.core.series import Series\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/series.py\", line 84, in <module>\n",
      "    import pandas.plotting\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/series.py\", line 4885, in <module>\n",
      "    Series._add_numeric_operations()\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.queues import feeding_functions\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/generic.py\", line 10680, in _add_numeric_operations\n",
      "    nanops.nanmedian,\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 857, in get_code\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/__init__.py\", line 147, in <module>\n",
      "    from pandas.io.api import (\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1 import estimator\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/plotting/__init__.py\", line 59, in <module>\n",
      "    from pandas.plotting._core import (\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/generic.py\", line 52, in <module>\n",
      "    from pandas.core.sparse.frame import SparseDataFrame\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator._api.v1.estimator import experimental\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\n",
      "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/plotting/_core.py\", line 17, in <module>\n",
      "    import pandas.plotting._matplotlib  # noqa\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\n",
      "    import tensorflow_estimator.python.estimator.estimator_lib\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/__init__.py\", line 3, in <module>\n",
      "    from pandas.plotting._matplotlib.boxplot import (\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 53, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs import inputs\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/inputs.py\", line 22, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.numpy_io import numpy_input_fn\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 525, in _compile_bytecode\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/io/api.py\", line 17, in <module>\n",
      "    from pandas.io.pytables import HDFStore, read_hdf\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/io/pytables.py\", line 53, in <module>\n",
      "    from pandas.core.computation.pytables import Expr, maybe_expression\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/computation/pytables.py\", line 16, in <module>\n",
      "    from pandas.core.computation import expr, ops\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator.inputs.queues import feeding_functions\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/computation/expr.py\", line 369, in <module>\n",
      "    class BaseExprVisitor(ast.NodeVisitor):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\n",
      "    import pandas as pd\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 857, in get_code\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/__init__.py\", line 55, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py\", line 14, in <module>\n",
      "    from pandas.plotting._matplotlib.core import LinePlot, MPLPlot\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 525, in _compile_bytecode\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/api.py\", line 24, in <module>\n",
      "    from pandas.core.groupby import Grouper, NamedAgg\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py\", line 32, in <module>\n",
      "    from pandas.plotting._matplotlib.compat import _mpl_ge_3_0_0\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.groupby.generic import (  # noqa: F401\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/groupby/generic.py\", line 44, in <module>\n",
      "    from pandas.core.frame import DataFrame\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/computation/expr.py\", line 319, in disallowed\n",
      "    cls.unsupported_nodes += (name,)\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/frame.py\", line 114, in <module>\n",
      "    from pandas.core.series import Series\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/core/series.py\", line 84, in <module>\n",
      "    import pandas.plotting\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/plotting/__init__.py\", line 59, in <module>\n",
      "    from pandas.plotting._core import (\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/plotting/_core.py\", line 17, in <module>\n",
      "    import pandas.plotting._matplotlib  # noqa\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/__init__.py\", line 3, in <module>\n",
      "    from pandas.plotting._matplotlib.boxplot import (\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n",
      "KeyboardInterrupt\n",
      "  File \"/home/pdeubel/.local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/boxplot.py\", line 4, in <module>\n",
      "    from matplotlib.artist import setp\n",
      "  File \"/usr/lib/python3.7/site-packages/matplotlib/__init__.py\", line 1099, in <module>\n",
      "    rcParams = rc_params()\n",
      "  File \"/usr/lib/python3.7/site-packages/matplotlib/__init__.py\", line 940, in rc_params\n",
      "    fname = matplotlib_fname()\n",
      "  File \"/usr/lib/python3.7/site-packages/matplotlib/__init__.py\", line 747, in matplotlib_fname\n",
      "    if os.path.exists(fname):\n",
      "  File \"/usr/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "env = gym.make(config.env_id)\n",
    "rs = np.random.RandomState()\n",
    "\n",
    "optimizer = Adam(stepsize=0.01)\n",
    "\n",
    "save_directory = \"/tmp/es_master_{}/\".format(os.getpid())\n",
    "\n",
    "mkdir_p(save_directory)\n",
    "\n",
    "ob_stat = RunningStat(\n",
    "    env.observation_space.shape,\n",
    "    eps=1e-2  # eps to prevent dividing by zero at the beginning when computing mean/stdev\n",
    ")\n",
    "\n",
    "tslimit, incr_tslimit_threshold, tslimit_incr_ratio = None, None, None\n",
    "adaptive_tslimit = False\n",
    "\n",
    "\n",
    "episodes_so_far = 0\n",
    "timesteps_so_far = 0\n",
    "tstart = time.time()\n",
    "\n",
    "task_counter = 0\n",
    "\n",
    "assert config.num_workers != 0\n",
    "\n",
    "num_jobs_per_worker = [int(config.population_size / config.num_workers)] * config.num_workers\n",
    "\n",
    "mod = config.population_size % config.num_workers\n",
    "i = 0\n",
    "while mod > 0:\n",
    "    num_jobs_per_worker[i] += 1\n",
    "    mod -= 1\n",
    "    i += 1\n",
    "    \n",
    "assert len(num_jobs_per_worker) == config.num_workers\n",
    "generation_counter = 0\n",
    "\n",
    "theta = get_flat(theta)\n",
    "\n",
    "while True:\n",
    "    print(\"----------------------GENERATION: \" + str(generation_counter) + \"------------------------------------\")\n",
    "    \n",
    "    step_tstart = time.time() \n",
    "            \n",
    "    #assert theta.dtype == np.float32\n",
    "\n",
    "    # Task counter is used to recognize false tasks from previous iterations later\n",
    "    curr_task_id = task_counter\n",
    "    task_counter += 1\n",
    "    \n",
    "    # Start workers\n",
    "    \n",
    "    workers = []\n",
    "    results = []\n",
    "    \n",
    "    pool = Pool(processes=config.num_workers)\n",
    "    \n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Waiting for results\")\n",
    "    for i in num_jobs_per_worker:\n",
    "        result = pool.apply_async(func=run_worker, args=(i, theta, ob_stat.mean, ob_stat.std))\n",
    "        results.append(result)\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        results[i] = results[i].get()\n",
    "\n",
    "    pool.close()   \n",
    "    pool.join()  \n",
    "\n",
    "    # Pop off results for the current task\n",
    "    curr_task_results, eval_rets, eval_lens, worker_ids = [], [], [], []\n",
    "    num_results_skipped, num_episodes_popped, num_timesteps_popped, ob_count_this_batch = 0, 0, 0, 0\n",
    "   #while num_episodes_popped < config.episodes_per_batch:\n",
    "    for result in results:\n",
    "        assert isinstance(result, Result)\n",
    "        # task_id = result.task_id\n",
    "        # assert isinstance(task_id, int)\n",
    "\n",
    "        # assert (result.eval_return is None) == (result.eval_length is None)\n",
    "        # worker_ids.append(result.worker_id)\n",
    "        # \n",
    "        # if result.eval_length is not None:\n",
    "        #     # This was an eval job\n",
    "        #     episodes_so_far += 1\n",
    "        #     timesteps_so_far += result.eval_length\n",
    "        #     # Store the result only for current tasks\n",
    "        #     if task_id == curr_task_id:\n",
    "        #         eval_rets.append(result.eval_return)\n",
    "        #         eval_lens.append(result.eval_length)\n",
    "        # else:\n",
    "\n",
    "        assert (result.noise_inds_n.ndim == 1 and\n",
    "                result.returns_n2.shape == result.lengths_n2.shape == (len(result.noise_inds_n), 2))\n",
    "        assert result.returns_n2.dtype == np.float32\n",
    "        \n",
    "        # Update counts\n",
    "        result_num_eps = result.lengths_n2.size\n",
    "        result_num_timesteps = result.lengths_n2.sum()\n",
    "        episodes_so_far += result_num_eps\n",
    "        timesteps_so_far += result_num_timesteps\n",
    "        # Store results only for current tasks\n",
    "        curr_task_results.append(result)\n",
    "        num_episodes_popped += result_num_eps\n",
    "        num_timesteps_popped += result_num_timesteps\n",
    "        # Update ob stats\n",
    "        if result.ob_count > 0:\n",
    "            ob_stat.increment(result.ob_sum, result.ob_sumsq, result.ob_count)\n",
    "            ob_count_this_batch += result.ob_count\n",
    "\n",
    "\n",
    "    # Compute skip fraction\n",
    "    #frac_results_skipped = num_results_skipped / (num_results_skipped + len(curr_task_results))\n",
    "    # if num_results_skipped > 0:\n",
    "    #     logger.warning('Skipped {} out of date results ({:.2f}%)'.format(\n",
    "    #         num_results_skipped, 100. * frac_results_skipped))\n",
    "    \n",
    "    print(\"Gathered results\")\n",
    "\n",
    "    # Assemble results\n",
    "    noise_inds_n = np.concatenate([r.noise_inds_n for r in curr_task_results])\n",
    "    returns_n2 = np.concatenate([r.returns_n2 for r in curr_task_results])\n",
    "    lengths_n2 = np.concatenate([r.lengths_n2 for r in curr_task_results])\n",
    "    assert noise_inds_n.shape[0] == returns_n2.shape[0] == lengths_n2.shape[0]\n",
    "    \n",
    "    # Process returns\n",
    "    if config.return_proc_mode == 'centered_rank':\n",
    "        proc_returns_n2 = compute_centered_ranks(returns_n2)\n",
    "    else:\n",
    "        proc_returns_n2 = returns_n2\n",
    "    \n",
    "    g, count = batched_weighted_sum(\n",
    "        proc_returns_n2[:, 0] - proc_returns_n2[:, 1],\n",
    "        (noise.get(idx, num_params) for idx in noise_inds_n),\n",
    "        batch_size=500\n",
    "    )\n",
    "    \n",
    "    g /= returns_n2.size\n",
    "    \n",
    "    #g /= config.noise_stdev\n",
    "    #g *= config.learning_rate\n",
    "    \n",
    "    assert g.shape == (num_params,) and g.dtype == np.float32 and count == len(noise_inds_n)\n",
    "    #update_ratio = optimizer.update(-g + config.l2coeff * theta)\n",
    "    #update_ratio = optimizer.update(config.l2coeff * g)\n",
    "\n",
    "    # UPDATE\n",
    "\n",
    "    theta, _ = optimizer.update(-g + config.learning_rate * theta)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #set_from_flat(master_model, theta + g)\n",
    "    #set_from_flat(master_model, theta + g)\n",
    "   \n",
    "    #theta += g\n",
    "\n",
    "    # Update ob stat (we're never running the policy in the master, but we might be snapshotting the policy)\n",
    "    # if policy.needs_ob_stat:\n",
    "    #     policy.set_ob_stat(ob_stat.mean, ob_stat.std)\n",
    "\n",
    "    # Update number of steps to take\n",
    "    # if adaptive_tslimit and (lengths_n2 == tslimit).mean() >= incr_tslimit_threshold:\n",
    "    #     old_tslimit = tslimit\n",
    "    #     tslimit = int(tslimit_incr_ratio * tslimit)\n",
    "    #     logger.info('Increased timestep limit from {} to {}'.format(old_tslimit, tslimit))\n",
    "\n",
    "    step_tend = time.time()\n",
    "    # tlogger.record_tabular(\"EpRewMean\", returns_n2.mean())\n",
    "    # tlogger.record_tabular(\"EpRewStd\", returns_n2.std())\n",
    "    # tlogger.record_tabular(\"EpLenMean\", lengths_n2.mean())\n",
    "    # \n",
    "    # tlogger.record_tabular(\"EvalEpRewMean\", np.nan if not eval_rets else np.mean(eval_rets))\n",
    "    # tlogger.record_tabular(\"EvalEpRewStd\", np.nan if not eval_rets else np.std(eval_rets))\n",
    "    # tlogger.record_tabular(\"EvalEpLenMean\", np.nan if not eval_rets else np.mean(eval_lens))\n",
    "    # tlogger.record_tabular(\"EvalPopRank\", np.nan if not eval_rets else (\n",
    "    #     np.searchsorted(np.sort(returns_n2.ravel()), eval_rets).mean() / returns_n2.size))\n",
    "    # tlogger.record_tabular(\"EvalEpCount\", len(eval_rets))\n",
    "    # \n",
    "    # tlogger.record_tabular(\"Norm\", float(np.square(policy.get_trainable_flat()).sum()))\n",
    "    # tlogger.record_tabular(\"GradNorm\", float(np.square(g).sum()))\n",
    "    # tlogger.record_tabular(\"UpdateRatio\", float(update_ratio))\n",
    "    # \n",
    "    # tlogger.record_tabular(\"EpisodesThisIter\", lengths_n2.size)\n",
    "    # tlogger.record_tabular(\"EpisodesSoFar\", episodes_so_far)\n",
    "    # tlogger.record_tabular(\"TimestepsThisIter\", lengths_n2.sum())\n",
    "    # tlogger.record_tabular(\"TimestepsSoFar\", timesteps_so_far)\n",
    "    # \n",
    "    # num_unique_workers = len(set(worker_ids))\n",
    "    # tlogger.record_tabular(\"UniqueWorkers\", num_unique_workers)\n",
    "    # tlogger.record_tabular(\"UniqueWorkersFrac\", num_unique_workers / len(worker_ids))\n",
    "    # tlogger.record_tabular(\"ResultsSkippedFrac\", frac_results_skipped)\n",
    "    # tlogger.record_tabular(\"ObCount\", ob_count_this_batch)\n",
    "    # \n",
    "    # tlogger.record_tabular(\"TimeElapsedThisIter\", step_tend - step_tstart)\n",
    "    # tlogger.record_tabular(\"TimeElapsed\", step_tend - tstart)\n",
    "    # tlogger.dump_tabular()\n",
    "\n",
    "    if config.snapshot_freq != 0 and generation_counter % config.snapshot_freq == 0:\n",
    "        from multiprocessing import Process\n",
    "        \n",
    "        p = Process(target=create_model, args=(theta, \n",
    "                                               config.env_id + \"_Generation_\" + str(generation_counter), \n",
    "                                               save_directory + 'snapshot_{:05d}'.format(generation_counter) + \".h5\"))\n",
    "        p.start()\n",
    "        p.join()\n",
    "        \n",
    "        print(\"Saved model in generation {}\".format(generation_counter))\n",
    "            \n",
    "    generation_counter+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "trained_model = create_model()\n",
    "trained_model.load_weights(save_directory + \"snapshot_0.h5\")\n",
    "\n",
    "\n",
    "import gym, roboschool\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "def rollout_rend(env, model, *, render=False, timestep_limit=None, save_obs=False, random_stream=None):\n",
    "    \"\"\"\n",
    "    If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "    Otherwise, no action noise will be added.\n",
    "    \"\"\"\n",
    "    \n",
    "    env_timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "    rews = []\n",
    "    t = 0\n",
    "    if save_obs:\n",
    "        obs = []\n",
    "    ob = env.reset()\n",
    "    for _ in range(timestep_limit):\n",
    "        ac = act(ob[None], model, random_stream=random_stream)[0]\n",
    "        if save_obs:\n",
    "            obs.append(ob)\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "        rews.append(rew)\n",
    "        t += 1\n",
    "        if render:\n",
    "            img.set_data(env.render(mode='rgb_array'))\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        if done:\n",
    "            break\n",
    "    rews = np.array(rews, dtype=np.float32)\n",
    "    if save_obs:\n",
    "        return rews, t, np.array(obs)\n",
    "    return rews, t\n",
    "\n",
    "#rollout_rend(env, trained_model, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Config = namedtuple('Config', [\n",
    "    'env_id',\n",
    "    'population_size',\n",
    "    'num_workers',\n",
    "    'learning_rate',\n",
    "    'noise_stdev',\n",
    "    'snapshot_freq',\n",
    "    'return_proc_mode'\n",
    "    #'timesteps_per_batch',\n",
    "    #'calc_obstat_prob',\n",
    "    #'eval_prob',\n",
    "    #'episode_cutoff_mode'\n",
    "])\n",
    "\n",
    "Result = namedtuple('Result', [\n",
    "    'worker_id',\n",
    "    'noise_inds_n','returns_n2', 'signreturns_n2', 'lengths_n2',\n",
    "    'eval_return', 'eval_length',\n",
    "    'ob_sum', 'ob_sumsq', 'ob_count',\n",
    "    'task_id'\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config = Config(\n",
    "    env_id=\"RoboschoolInvertedPendulum-v1\",\n",
    "    population_size=8,\n",
    "    num_workers=1,\n",
    "    learning_rate=0.005,\n",
    "    noise_stdev=0.02,\n",
    "    snapshot_freq=20,\n",
    "    return_proc_mode=\"centered_rank\"\n",
    ")\n",
    "\n",
    "#config.num_workers = config.num_workers if config.num_workers else os.cpu_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment\n",
    "\n",
    "Create one for every worker -> done in worker method\n",
    "Master also"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import gym, roboschool # Roboschool import needed to register the environments within gym\n",
    "env = gym.make(config.env_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorflow Session"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n  warnings.warn('An interactive session is already active. This can '\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy setup\n",
    "\n",
    "Currently saves the arguments as local variable, then creates a TensorFlow variable scope where the neural network\n",
    "architecture gets created.\n",
    "\n",
    "Currently emitted:\n",
    "1. Observation normalization\n",
    "2. Obseration clipping\n",
    "3. _act function\n",
    "6. set_all_vars\n",
    "\n",
    "## Keras as Model\n",
    "\n",
    "Original implementation used hand written dense layers and tensorflow operations. I use a Keras model and their\n",
    "functional API to create the net. In testing the two version differ in 0.x float scope. Something to worry about?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "args = {\n",
    "      \"ac_bins\": \"continuous:\",\n",
    "      \"ac_noise_std\": 0.01,\n",
    "      #\"connection_type\": \"ff\",\n",
    "      \"hidden_dims\": [\n",
    "        256,\n",
    "        256\n",
    "      ],\n",
    "      \"nonlin_type\": \"tanh\"\n",
    "}\n",
    "\n",
    "ob_space= env.observation_space\n",
    "ac_space = env.action_space\n",
    "ac_bins = args[\"ac_bins\"]\n",
    "ac_noise_std = args[\"ac_noise_std\"]\n",
    "hidden_dims = args[\"hidden_dims\"]\n",
    "nonlin = args[\"nonlin_type\"]\n",
    "\n",
    "# TODO more nonlinear functions\n",
    "nonlin = tf.tanh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras clearin backend to support multiprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "PID 14046: Model entry\nPID 14046: test\nPID 14046: Model out\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def create_model(initial_weights=None, model_name=\"model\"):\n",
    "    #tf.keras.backend.clear_session()\n",
    "\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Model entry\")\n",
    "    with tf.variable_scope(\"RoboschoolPolicy/\" + model_name):\n",
    "        # Observation normalization\n",
    "        #ob_mean = tf.get_variable(\n",
    "        #    'ob_mean', ob_space.shape, tf.float32, tf.constant_initializer(np.nan), trainable=False)\n",
    "        #ob_std = tf.get_variable(\n",
    "        #    'ob_std', ob_space.shape, tf.float32, tf.constant_initializer(np.nan), trainable=False)\n",
    "        #in_mean = tf.placeholder(tf.float32, ob_space.shape)\n",
    "        #in_std = tf.placeholder(tf.float32, ob_space.shape)\n",
    "        #self._set_ob_mean_std = U.function([in_mean, in_std], [], updates=[\n",
    "            #tf.assign(ob_mean, in_mean),\n",
    "            #tf.assign(ob_std, in_std),\n",
    "        #])\n",
    "\n",
    "        # Normalize observation space and clip to [-5.0, 5.0]\n",
    "        #o = tf.clip_by_value((o - ob_mean) / ob_std, -5.0, 5.0)\n",
    "\n",
    "        # Policy network\n",
    "\n",
    "        input = x = tf.keras.Input(ob_space.shape, dtype=tf.float32)\n",
    "\n",
    "        for hd in hidden_dims:\n",
    "            x = tf.keras.layers.Dense(\n",
    "                hd, activation=nonlin,\n",
    "                kernel_initializer=tf.initializers.ones,\n",
    "                bias_initializer=tf.initializers.zeros)(x)\n",
    "\n",
    "        # Map to action\n",
    "        adim = ac_space.shape[0]\n",
    "\n",
    "        a = tf.keras.layers.Dense(\n",
    "        adim,\n",
    "        kernel_initializer=tf.initializers.ones,\n",
    "        bias_initializer=tf.initializers.zeros)(x)\n",
    "        model = tf.keras.Model(inputs=input, outputs=a, name=model_name)\n",
    "\n",
    "        # Initializer for the newly created weights. TODO possible replacement tf.keras.initializers.RandomNormal\n",
    "        # out = np.random.randn(*adim).astype(np.float32)\n",
    "        # out *=  0.01 / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        # initializer= tf.constant(out)\n",
    "\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"test\")\n",
    "    if initial_weights is not None:\n",
    "        print(\"PID \" + str(os.getpid()) + \": \" \"Further test\")\n",
    "        set_from_flat(model, initial_weights)\n",
    "\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Model out\")    \n",
    "    return model\n",
    "\n",
    "master_model = create_model(model_name=\"master\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 5)                 0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 256)               1536      \n_________________________________________________________________\ndense_4 (Dense)              (None, 256)               65792     \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 257       \n=================================================================\nTotal params: 67,585\nTrainable params: 67,585\nNon-trainable params: 0\n_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def act(ob, model, random_stream=None):\n",
    "    return model.predict(ob[None])\n",
    "    \n",
    "\n",
    "# Plot the Neural Network Architecture\n",
    "master_model.summary()\n",
    "#all_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope.name)\n",
    "\n",
    "\n",
    "#trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, master_scope.name)\n",
    "#trainable_variables = model.get_weights()\n",
    "\n",
    "num_params = sum(np.prod(v.shape) for v in master_model.get_weights())\n",
    "\n",
    "#placeholders = [tf.placeholder(v.value().dtype, v.get_shape().as_list()) for v in self.all_variables]\n",
    "\n",
    "# self.set_all_vars = U.function(\n",
    "#     inputs=placeholders,\n",
    "#     outputs=[],\n",
    "#     updates=[tf.group(*[v.assign(p) for v, p in zip(self.all_variables, placeholders)])]\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "#optimizer = {'sgd': SGD, 'adam': Adam}[exp['optimizer']['type']](policy, **exp['optimizer']['args'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shared Noise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class SharedNoiseTable(object):\n",
    "    def __init__(self):\n",
    "        import ctypes, multiprocessing\n",
    "        seed = 123\n",
    "        count = 250000000  # 1 gigabyte of 32-bit numbers. Will actually sample 2 gigabytes below.\n",
    "        #logger.info('Sampling {} random numbers with seed {}'.format(count, seed))\n",
    "\n",
    "        # Instantiate an array of C float datatype with size count\n",
    "        self._shared_mem = multiprocessing.Array(ctypes.c_float, count)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        self.noise = np.ctypeslib.as_array(self._shared_mem.get_obj())\n",
    "        assert self.noise.dtype == np.float32\n",
    "        self.noise[:] = np.random.RandomState(seed).randn(count)  # 64-bit to 32-bit conversion here\n",
    "        #logger.info('Sampled {} bytes'.format(self.noise.size * 4))\n",
    "\n",
    "    def get(self, i, dim):\n",
    "        return self.noise[i:i + dim]\n",
    "\n",
    "    def sample_index(self, stream, dim):\n",
    "        return stream.randint(0, len(self.noise) - dim + 1)\n",
    "\n",
    "noise = SharedNoiseTable()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get flat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# def set_new_weights(model, theta, epsilon):\n",
    "#     assert isinstance(model, tf.keras.Model)\n",
    "#     assert isinstance(theta, list)\n",
    "#         \n",
    "#     for t in theta:\n",
    "#         t += epsilon\n",
    "#     \n",
    "#     model.set_weights(theta)\n",
    "\n",
    "def get_flat(theta):\n",
    "     return np.concatenate([np.reshape(v, [-1]) for v in theta], 0)\n",
    "\n",
    "def set_from_flat(model, theta):\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Set from init\")\n",
    "    old_theta = model.get_weights()\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"WHAAT\")\n",
    "    shapes = [v.shape for v in old_theta]\n",
    "    total_size = theta.size\n",
    "    \n",
    "    start = 0\n",
    "    reshapes = []\n",
    "    \n",
    "    for (shape, v) in zip(shapes, theta):\n",
    "        size = int(np.prod(shape))\n",
    "        reshapes.append(np.reshape(theta[start:start+size], shape))\n",
    "        start += size\n",
    "    \n",
    "    assert start == total_size\n",
    "    \n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"set from init end before update\")\n",
    "    model.set_weights(reshapes)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set from flat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# def _create_set_from_flat_op(var_list, orig):\n",
    "#     shapes = [v.shape for v in orig]\n",
    "#     total_size = np.sum([v.size for v in orig])\n",
    "#         \n",
    "#     start=0\n",
    "#     assigns = []\n",
    "#     for (shape, v) in zip(shapes, var_list):\n",
    "#         size = v.size\n",
    "#         assigns.append(np.reshape(var_list[start:start+size], shape))\n",
    "#         start += size\n",
    "#         \n",
    "#     assert start == total_size\n",
    "#     \n",
    "#     return assigns\n",
    "#  \n",
    "# \n",
    "# def set_from_flat(var_list):\n",
    "#     old_weights = get_flat(model.get_weights())\n",
    "# \n",
    "#     new_weights = old_weights + var_list\n",
    "#     op_set_from_flat = _create_set_from_flat_op(new_weights)\n",
    "#     model.set_weights(sess.run(op_set_from_flat))\n",
    "#     \n",
    "#     print(\"PID \" + str(os.getpid()) + \": \" + \"Set weights from flat\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rollout TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def rollout(env, model, *, render=False, timestep_limit=None, save_obs=False, random_stream=None):\n",
    "    \"\"\"\n",
    "    If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "    Otherwise, no action noise will be added.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Rollout\")\n",
    "    \n",
    "    env_timestep_limit = env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
    "    timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "    rews = []\n",
    "    t = 0\n",
    "    if save_obs:\n",
    "        obs = []\n",
    "    ob = env.reset()\n",
    "    for _ in range(timestep_limit):\n",
    "        ac = act(ob[None], model, random_stream=random_stream)[0]\n",
    "        if save_obs:\n",
    "            obs.append(ob)\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "        rews.append(rew)\n",
    "        t += 1\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "    rews = np.array(rews, dtype=np.float32)\n",
    "    if save_obs:\n",
    "        return rews, t, np.array(obs)\n",
    "    return rews, t\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Worker method\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def run_worker(num_jobs, theta): #min_task_runtime=.2):\n",
    "\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Started worker with \" + str(num_jobs) + \"Jobs\")\n",
    "    #with lock:\n",
    "    #    logger.info('run_worker: {}'.format(locals()))\n",
    "\n",
    "    assert isinstance(noise, SharedNoiseTable)\n",
    "\n",
    "    # Setup\n",
    "    #config, env, sess, policy = setup(exp, single_threaded=True)\n",
    "    env = gym.make(config.env_id)\n",
    "    model = create_model(initial_weights=theta, model_name=str(os.getpid()))\n",
    "    \n",
    "\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Created model\")\n",
    "    # Random stream used for todo\n",
    "    rs = np.random.RandomState()\n",
    "    #worker_id = rs.randint(2 ** 31)\n",
    "\n",
    "    #assert policy.needs_ob_stat == (config.calc_obstat_prob != 0)\n",
    "\n",
    "    #while True:\n",
    "    # Prevent accessing empty array (master did not emit task yet)\n",
    "    #while not tasks:\n",
    "    #    time.sleep(0.05)\n",
    "\n",
    "    #task_data = tasks[-1]\n",
    "\n",
    "    #task_tstart = time.time()\n",
    "\n",
    "    #assert isinstance(task_data, Task)\n",
    "    #task_id = task_data.task_id\n",
    "    #assert isinstance(task_id, int)\n",
    "\n",
    "    #if policy.needs_ob_stat:\n",
    "    #    policy.set_ob_stat(task_data.ob_mean, task_data.ob_std)\n",
    "\n",
    "    # # todo whats this condition doing?\n",
    "    # if rs.rand() < config.eval_prob:\n",
    "    #     # Evaluation: noiseless weights and noiseless actions\n",
    "    #     policy.set_trainable_flat(task_data.params)\n",
    "    # \n",
    "    #     eval_rews, eval_length = policy.rollout(env)  # eval rollouts don't obey task_data.timestep_limit\n",
    "    #     eval_return = eval_rews.sum()\n",
    "    # \n",
    "    #     with lock:\n",
    "    #         logger.info('Eval result: task={} return={:.3f} length={}'.format(task_id, eval_return, eval_length))\n",
    "    # \n",
    "    #     result_queue.put(Result(\n",
    "    #         worker_id=worker_id,\n",
    "    #         noise_inds_n=None,\n",
    "    #         returns_n2=None,\n",
    "    #         signreturns_n2=None,\n",
    "    #         lengths_n2=None,\n",
    "    #         eval_return=eval_return,\n",
    "    #         eval_length=eval_length,\n",
    "    #         ob_sum=None,\n",
    "    #         ob_sumsq=None,\n",
    "    #         ob_count=None,\n",
    "    #         task_id=task_id\n",
    "    #     ))\n",
    "\n",
    "    # Rollouts with noise\n",
    "    noise_inds, returns, signreturns, lengths = [], [], [], []\n",
    "    #task_ob_stat = RunningStat(env.observation_space.shape, eps=0.)  # eps=0 because we're incrementing only\n",
    "    \n",
    "    #while not noise_inds or time.time() - task_tstart < min_task_runtime:\n",
    "    \n",
    "    for _ in range(num_jobs):\n",
    "        print(\"PID \" + str(os.getpid()) + \": \" + \"Iteration done\")\n",
    "        # ------------- Noise sample -------------------------------\n",
    "        noise_idx = noise.sample_index(rs, num_params)\n",
    "        epsilon = config.noise_stdev * noise.get(noise_idx, num_params)\n",
    "        print(epsilon.shape)\n",
    "\n",
    "        print(\"PID \" + str(os.getpid()) + \": \" + \"Sampled noise\")\n",
    "        # Evaluate the sampled noise positive\n",
    "        set_from_flat(model, theta + epsilon)\n",
    "        rews_pos, len_pos = rollout(env, model)\n",
    "\n",
    "        print(\"PID \" + str(os.getpid()) + \": \" + \"Positive sample\")\n",
    "        # rews_pos, len_pos = rollout_and_update_ob_stat(\n",
    "        #     policy, env, task_data.timestep_limit, rs, task_ob_stat, config.calc_obstat_prob)\n",
    "        \n",
    "        # Evaluate the sample noise negative\n",
    "        set_from_flat(model, theta - epsilon)\n",
    "        rews_neg, len_neg = rollout(env, model)\n",
    "        \n",
    "        print(\"PID \" + str(os.getpid()) + \": \" + \"Negative sample\")\n",
    "        # rews_neg, len_neg = rollout_and_update_ob_stat(\n",
    "        #     policy, env, task_data.timestep_limit, rs, task_ob_stat, config.calc_obstat_prob)\n",
    "        \n",
    "    \n",
    "        # Gather results\n",
    "        noise_inds.append(noise_idx)\n",
    "        returns.append([rews_pos.sum(), rews_neg.sum()])\n",
    "        signreturns.append([np.sign(rews_pos).sum(), np.sign(rews_neg).sum()])\n",
    "        lengths.append([len_pos, len_neg])\n",
    "        \n",
    "        print(\"PID \" + str(os.getpid()) + \": \" + \"Appended results\")\n",
    "        \n",
    "    # result_queue.put(Result(\n",
    "    #     worker_id=worker_id,\n",
    "    #     noise_inds_n=np.array(noise_inds),\n",
    "    #     returns_n2=np.array(returns, dtype=np.float32),\n",
    "    #     signreturns_n2=np.array(signreturns, dtype=np.float32),\n",
    "    #     lengths_n2=np.array(lengths, dtype=np.int32),\n",
    "    #     eval_return=None,\n",
    "    #     eval_length=None,\n",
    "    #     ob_sum=None if task_ob_stat.count == 0 else task_ob_stat.sum,\n",
    "    #     ob_sumsq=None if task_ob_stat.count == 0 else task_ob_stat.sumsq,\n",
    "    #     ob_count=task_ob_stat.count,\n",
    "    #     task_id=task_id\n",
    "    # ))\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Returned result\")\n",
    "    result = Result(\n",
    "        worker_id=None,\n",
    "        noise_inds_n=noise_inds,\n",
    "        returns_n2=returns,\n",
    "        signreturns_n2=signreturns,\n",
    "        lengths_n2=lengths,\n",
    "        eval_return=None,\n",
    "        eval_length=None,\n",
    "        ob_sum=None,\n",
    "        ob_count=None,\n",
    "        ob_sumsq=None,\n",
    "        task_id = 0\n",
    "    )\n",
    "    \n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def itergroups(items, group_size):\n",
    "    assert group_size >= 1\n",
    "    group = []\n",
    "    for x in items:\n",
    "        group.append(x)\n",
    "        if len(group) == group_size:\n",
    "            yield tuple(group)\n",
    "            del group[:]\n",
    "    if group:\n",
    "        yield tuple(group)\n",
    "        \n",
    "def batched_weighted_sum(weights, vecs, batch_size):\n",
    "    total = 0.\n",
    "    num_items_summed = 0\n",
    "    for batch_weights, batch_vecs in zip(itergroups(weights, batch_size), itergroups(vecs, batch_size)):\n",
    "        assert len(batch_weights) == len(batch_vecs) <= batch_size\n",
    "        total += np.dot(np.asarray(batch_weights, dtype=np.float32), np.asarray(batch_vecs, dtype=np.float32))\n",
    "        num_items_summed += len(batch_weights)\n",
    "    return total, num_items_summed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Master"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Generation: 0\n",
      "PID 14723: Started worker with 8Jobs\n",
      "PID 14723: Model entry\n",
      "PID 14723: test\n",
      "PID 14723: Further test\n",
      "PID 14723: Set from init\n",
      "PID 14046: Waiting for results\nPID 14046: Waiting for 0\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-44355a05e199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PID \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Waiting for \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "env = gym.make(config.env_id)\n",
    "rs = np.random.RandomState()\n",
    "\n",
    "\n",
    "# ob_stat = RunningStat(\n",
    "#     env.observation_space.shape,\n",
    "#     eps=1e-2  # eps to prevent dividing by zero at the beginning when computing mean/stdev\n",
    "# )\n",
    "\n",
    "tslimit, incr_tslimit_threshold, tslimit_incr_ratio = None, None, None\n",
    "adaptive_tslimit = False\n",
    "\n",
    "\n",
    "episodes_so_far = 0\n",
    "timesteps_so_far = 0\n",
    "tstart = time.time()\n",
    "\n",
    "task_counter = 0\n",
    "\n",
    "assert config.num_workers != 0\n",
    "\n",
    "num_jobs_per_worker = [int(config.population_size / config.num_workers)] * config.num_workers\n",
    "\n",
    "mod = config.population_size % config.num_workers\n",
    "i = 0\n",
    "while mod > 0:\n",
    "    num_jobs_per_worker[i] += 1\n",
    "    mod -= 1\n",
    "    i += 1\n",
    "    \n",
    "assert len(num_jobs_per_worker) == config.num_workers\n",
    "generation_counter = 0\n",
    "\n",
    "while True:\n",
    "    print(\"Generation: \" + str(generation_counter))\n",
    "    \n",
    "    step_tstart = time.time()\n",
    "    \n",
    "    theta = get_flat(master_model.get_weights())\n",
    "            \n",
    "    #assert theta.dtype == np.float32\n",
    "\n",
    "    # Task counter is used to recognize false tasks from previous iterations later\n",
    "    curr_task_id = task_counter\n",
    "    task_counter += 1\n",
    "\n",
    "    # tasks.append(Task(\n",
    "    #         params=theta,\n",
    "    #         ob_mean=ob_stat.mean if policy.needs_ob_stat else None,\n",
    "    #         ob_std=ob_stat.std if policy.needs_ob_stat else None,\n",
    "    #         timestep_limit=tslimit,\n",
    "    #         task_id = curr_task_id\n",
    "    # ))\n",
    "    \n",
    "    # Start workers\n",
    "    \n",
    "    workers = []\n",
    "    results = []\n",
    "    \n",
    "    pool = Pool(processes=config.num_workers)\n",
    "    \n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Waiting for results\")\n",
    "    for i in num_jobs_per_worker:\n",
    "        result = pool.apply_async(func=run_worker, args=(i, theta))\n",
    "        results.append(result)\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        print(\"PID \" + str(os.getpid()) + \": \" + \"Waiting for \" + str(i))\n",
    "        results[i] = results[i].get()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    pool.close()\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Pool closed\")\n",
    "    \n",
    "    pool.join()\n",
    "    print(\"PID \" + str(os.getpid()) + \": \" + \"Pool joined\")\n",
    "    \n",
    "\n",
    "    # Pop off results for the current task\n",
    "    curr_task_results, eval_rets, eval_lens, worker_ids = [], [], [], []\n",
    "    num_results_skipped, num_episodes_popped, num_timesteps_popped, ob_count_this_batch = 0, 0, 0, 0\n",
    "   #while num_episodes_popped < config.episodes_per_batch:\n",
    "    for result in results:\n",
    "        assert isinstance(result, Result)\n",
    "        # task_id = result.task_id\n",
    "        # assert isinstance(task_id, int)\n",
    "\n",
    "        # assert (result.eval_return is None) == (result.eval_length is None)\n",
    "        # worker_ids.append(result.worker_id)\n",
    "        # \n",
    "        # if result.eval_length is not None:\n",
    "        #     # This was an eval job\n",
    "        #     episodes_so_far += 1\n",
    "        #     timesteps_so_far += result.eval_length\n",
    "        #     # Store the result only for current tasks\n",
    "        #     if task_id == curr_task_id:\n",
    "        #         eval_rets.append(result.eval_return)\n",
    "        #         eval_lens.append(result.eval_length)\n",
    "        # else:\n",
    "        # The real shit\n",
    "        assert (result.noise_inds_n.ndim == 1 and\n",
    "                result.returns_n2.shape == result.lengths_n2.shape == (len(result.noise_inds_n), 2))\n",
    "        assert result.returns_n2.dtype == np.float32\n",
    "        \n",
    "        # Update counts\n",
    "        result_num_eps = result.lengths_n2.size\n",
    "        result_num_timesteps = result.lengths_n2.sum()\n",
    "        episodes_so_far += result_num_eps\n",
    "        timesteps_so_far += result_num_timesteps\n",
    "        # Store results only for current tasks\n",
    "        curr_task_results.append(result)\n",
    "        num_episodes_popped += result_num_eps\n",
    "        num_timesteps_popped += result_num_timesteps\n",
    "        # Update ob stats\n",
    "        # if policy.needs_ob_stat and result.ob_count > 0:\n",
    "        #     ob_stat.increment(result.ob_sum, result.ob_sumsq, result.ob_count)\n",
    "        #     ob_count_this_batch += result.ob_count\n",
    "\n",
    "\n",
    "    # Compute skip fraction\n",
    "    #frac_results_skipped = num_results_skipped / (num_results_skipped + len(curr_task_results))\n",
    "    # if num_results_skipped > 0:\n",
    "    #     logger.warning('Skipped {} out of date results ({:.2f}%)'.format(\n",
    "    #         num_results_skipped, 100. * frac_results_skipped))\n",
    "    \n",
    "    print(\"Gathered results\")\n",
    "\n",
    "    # Assemble results\n",
    "    noise_inds_n = np.concatenate([r.noise_inds_n for r in curr_task_results])\n",
    "    returns_n2 = np.concatenate([r.returns_n2 for r in curr_task_results])\n",
    "    lengths_n2 = np.concatenate([r.lengths_n2 for r in curr_task_results])\n",
    "    assert noise_inds_n.shape[0] == returns_n2.shape[0] == lengths_n2.shape[0]\n",
    "    \n",
    "    # Process returns\n",
    "    # if config.return_proc_mode == 'centered_rank':\n",
    "    #     proc_returns_n2 = compute_centered_ranks(returns_n2)\n",
    "    # elif config.return_proc_mode == 'sign':\n",
    "    #     proc_returns_n2 = np.concatenate([r.signreturns_n2 for r in curr_task_results])\n",
    "    # elif config.return_proc_mode == 'centered_sign_rank':\n",
    "    #     proc_returns_n2 = compute_centered_ranks(np.concatenate([r.signreturns_n2 for r in curr_task_results]))\n",
    "    # else:\n",
    "    #     raise NotImplementedError(config.return_proc_mode)\n",
    "    # Compute and take step\n",
    "    \n",
    "    proc_returns_n2 = returns_n2\n",
    "    \n",
    "    g, count = batched_weighted_sum(\n",
    "        proc_returns_n2[:, 0] - proc_returns_n2[:, 1],\n",
    "        (noise.get(idx, num_params) for idx in noise_inds_n),\n",
    "        batch_size=500\n",
    "    )\n",
    "    g /= returns_n2.size\n",
    "    g /= config.noise_stdev\n",
    "    g *= config.l2coeff\n",
    "    \n",
    "    assert g.shape == (num_params,) and g.dtype == np.float32 and count == len(noise_inds_n)\n",
    "    #update_ratio = optimizer.update(-g + config.l2coeff * theta)\n",
    "    #update_ratio = optimizer.update(config.l2coeff * g)\n",
    "\n",
    "    # UPDATE\n",
    "    \n",
    "    updated_weights = []\n",
    "\n",
    "    #set_from_flat(master_model, theta + g)\n",
    "    set_from_flat(master_model, theta + g)\n",
    "   \n",
    "\n",
    "    # Update ob stat (we're never running the policy in the master, but we might be snapshotting the policy)\n",
    "    # if policy.needs_ob_stat:\n",
    "    #     policy.set_ob_stat(ob_stat.mean, ob_stat.std)\n",
    "\n",
    "    # Update number of steps to take\n",
    "    # if adaptive_tslimit and (lengths_n2 == tslimit).mean() >= incr_tslimit_threshold:\n",
    "    #     old_tslimit = tslimit\n",
    "    #     tslimit = int(tslimit_incr_ratio * tslimit)\n",
    "    #     logger.info('Increased timestep limit from {} to {}'.format(old_tslimit, tslimit))\n",
    "\n",
    "    step_tend = time.time()\n",
    "    # tlogger.record_tabular(\"EpRewMean\", returns_n2.mean())\n",
    "    # tlogger.record_tabular(\"EpRewStd\", returns_n2.std())\n",
    "    # tlogger.record_tabular(\"EpLenMean\", lengths_n2.mean())\n",
    "    # \n",
    "    # tlogger.record_tabular(\"EvalEpRewMean\", np.nan if not eval_rets else np.mean(eval_rets))\n",
    "    # tlogger.record_tabular(\"EvalEpRewStd\", np.nan if not eval_rets else np.std(eval_rets))\n",
    "    # tlogger.record_tabular(\"EvalEpLenMean\", np.nan if not eval_rets else np.mean(eval_lens))\n",
    "    # tlogger.record_tabular(\"EvalPopRank\", np.nan if not eval_rets else (\n",
    "    #     np.searchsorted(np.sort(returns_n2.ravel()), eval_rets).mean() / returns_n2.size))\n",
    "    # tlogger.record_tabular(\"EvalEpCount\", len(eval_rets))\n",
    "    # \n",
    "    # tlogger.record_tabular(\"Norm\", float(np.square(policy.get_trainable_flat()).sum()))\n",
    "    # tlogger.record_tabular(\"GradNorm\", float(np.square(g).sum()))\n",
    "    # tlogger.record_tabular(\"UpdateRatio\", float(update_ratio))\n",
    "    # \n",
    "    # tlogger.record_tabular(\"EpisodesThisIter\", lengths_n2.size)\n",
    "    # tlogger.record_tabular(\"EpisodesSoFar\", episodes_so_far)\n",
    "    # tlogger.record_tabular(\"TimestepsThisIter\", lengths_n2.sum())\n",
    "    # tlogger.record_tabular(\"TimestepsSoFar\", timesteps_so_far)\n",
    "    # \n",
    "    # num_unique_workers = len(set(worker_ids))\n",
    "    # tlogger.record_tabular(\"UniqueWorkers\", num_unique_workers)\n",
    "    # tlogger.record_tabular(\"UniqueWorkersFrac\", num_unique_workers / len(worker_ids))\n",
    "    # tlogger.record_tabular(\"ResultsSkippedFrac\", frac_results_skipped)\n",
    "    # tlogger.record_tabular(\"ObCount\", ob_count_this_batch)\n",
    "    # \n",
    "    # tlogger.record_tabular(\"TimeElapsedThisIter\", step_tend - step_tstart)\n",
    "    # tlogger.record_tabular(\"TimeElapsed\", step_tend - tstart)\n",
    "    # tlogger.dump_tabular()\n",
    "\n",
    "    # if config.snapshot_freq != 0 and curr_task_id % config.snapshot_freq == 0:\n",
    "    #     import os.path as osp\n",
    "    #     filename = osp.join(tlogger.get_dir(), 'snapshot_iter{:05d}_rew{}.h5'.format(\n",
    "    #         curr_task_id,\n",
    "    #         np.nan if not eval_rets else int(np.mean(eval_rets))\n",
    "    #     ))\n",
    "    #     assert not osp.exists(filename)\n",
    "    #     policy.save(filename)\n",
    "    #     tlogger.log('Saved snapshot {}'.format(filename))\n",
    "    \n",
    "    generation_counter+= 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}